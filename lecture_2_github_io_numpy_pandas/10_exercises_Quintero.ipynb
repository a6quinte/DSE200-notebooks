{
 "metadata": {
  "name": "",
  "signature": "sha256:287b2f855cb284dcefeb29c0e165d65472e412f8e3579089fa3847bf8b962363"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Week 2 Exercises\n",
      "\n",
      "In this weeks exercises you will use Numpy/Scipy to impliment some numerical algorithms and then you will use Pandas to perform a rudamentary data analysis using the KDD 98 dataset.  Along the way you will use unix/basic python from the first week as well as git to save your work.\n",
      "\n",
      "As a first step we import the libraries we'll use later on.  This allows us to use numpy library calls by prefixing the call with np."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import the libraries \n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import pandas as pd\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Matrix Manipulations\n",
      "Lets first create a matrix and perform some manipulations of it.\n",
      "\n",
      "Using numpy's matrix data structure, define the following matricies:\n",
      "\n",
      "$$A=\\left[ \\begin{array}{ccc} 3 & 5 & 9 \\\\ 3 & 3 & 4 \\\\ 5 & 9 & 17 \\end{array} \\right]$$\n",
      "\n",
      "$$B=\\left[ \\begin{array}{c} 2 \\\\ 1 \\\\ 4 \\end{array} \\right]$$\n",
      "\n",
      "After this solve the matrix equation:\n",
      "$$Ax = B$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.array([[3,5,9], [3,3,4],[5,9,17]], dtype=float)\n",
      "b = np.array([2,1,4], dtype=float)\n",
      "x = np.linalg.solve(a,b)\n",
      "x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "array([ 1., -2.,  1.])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now write three functions for matrix multiply $C=AB$ in each of the following styles:\n",
      "\n",
      "1. By using nested for loops to impliment the naive algorithm ($C_{ij}=\\sum_{k=0}^{m-1}A_{ik}B_{kj}$)\n",
      "2. Using numpy's built in martrix multiplication  \n",
      "3. Using Cython\n",
      "\n",
      "The three methods should have the same answer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Nested loop method:\n",
      "# \n",
      "# loop over each row of A, zipping it with B\n",
      "\n",
      "aa = np.array([[3,2,4],[7,1,3],[9,7,7],[1,2,4],[6,7,8]], dtype=float)\n",
      "bb = np.array([[2,1],[3,5],[1,1]], dtype=float)\n",
      "\n",
      "def A_loopMult(AA,BB):\n",
      "    outMatrix = []        # the returned matrix\n",
      "    for rowA in AA:       # loop over Rows\n",
      "        outRow = []       \n",
      "        for colB in BB.transpose(): # loop over columns\n",
      "            # 'zip' and list comprehension\n",
      "            outRow.append( sum(ii*jj for ii,jj in zip(rowA,colB)) )\n",
      "        outMatrix.append(outRow)\n",
      "    return outMatrix\n",
      "\n",
      "A_loopMult(aa,bb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[[16.0, 17.0], [20.0, 15.0], [46.0, 51.0], [12.0, 15.0], [41.0, 49.0]]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Numpy method:\n",
      "def B_numpyMult(aa,bb): return aa.dot(bb)\n",
      "\n",
      "B_numpyMult(aa,bb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([[ 16.,  17.],\n",
        "       [ 20.,  15.],\n",
        "       [ 46.,  51.],\n",
        "       [ 12.,  15.],\n",
        "       [ 41.,  49.]])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext cythonmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cython\n",
      "cimport numpy as np\n",
      "# same as above, but using Cython\n",
      "def C_cythonMult(np.ndarray[np.float64_t, ndim=2] AA, np.ndarray[np.float64_t, ndim=2] BB):\n",
      "    outMatrix = []        # the returned matrix\n",
      "    for rowA in AA:       # loop over Rows\n",
      "        outRow = []       \n",
      "        for colB in BB.transpose(): # loop over columns\n",
      "            # 'zip' and list comprehension\n",
      "            outRow.append( sum(ii*jj for ii,jj in zip(rowA,colB)) )\n",
      "        outMatrix.append(outRow)\n",
      "    return outMatrix\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C_cythonMult(aa,bb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[[16.0, 17.0], [20.0, 15.0], [46.0, 51.0], [12.0, 15.0], [41.0, 49.0]]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we wish to evaluate the performance of these three methods.  Write a method that given three dmiensions (a,b,c) makes a random a x b and b x c matrix and computes the product using your three functions and reports the speed of each method.\n",
      "\n",
      "After this measure performance of each method for all $a,b,c \\in \\{10,100,1000,10000\\}$ and plot the results.  Is one method always the fastest?  Discuss why this is or is not the case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nRuns = 10\n",
      "sizeArr = [10**ii for ii in range(1,3)]\n",
      "\n",
      "# 100\n",
      "a = b = c = sizeArr[1]\n",
      "\n",
      "# for thisSize in sizeArr:\n",
      "#     # run test nRuns times for each method\n",
      "#     timesA, timesB, timesC = [],[],[]\n",
      "#     for runNum in range(nRuns):\n",
      "#         print \"Matrix sizes:\" thisSize, \" | Run number:\", runNum\n",
      "\n",
      "#         aa = np.random.rand(a,b)\n",
      "#         bb = np.random.rand(b,c)\n",
      "\n",
      "#         # Method A: Loop\n",
      "#         tic = time.time()\n",
      "#         calcOutput = A_loopMult(aa,bb)\n",
      "#         toc = time.time()\n",
      "#         timesA.append(toc-tic)\n",
      "\n",
      "#         # Method B: Numpy\n",
      "#         tic = time.time()\n",
      "#         calcOutput = B_numpyMult(aa,bb)\n",
      "#         toc = time.time()\n",
      "#         timesB.append(toc-tic)\n",
      "\n",
      "#         # Method C: Cython\n",
      "#         tic = time.time()\n",
      "#         calcOutput = C_cythonMult(aa,bb)\n",
      "#         toc = time.time()\n",
      "#         timesC.append(toc-tic)\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tdf = pd.DataFrame({'a':timesA, 'b':timesB, 'c':timesC})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tdf\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>a</th>\n",
        "      <th>b</th>\n",
        "      <th>c</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> 1.387132</td>\n",
        "      <td> 0.002991</td>\n",
        "      <td> 1.240450</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 1.383202</td>\n",
        "      <td> 0.002530</td>\n",
        "      <td> 1.225369</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 1.374979</td>\n",
        "      <td> 0.002628</td>\n",
        "      <td> 1.226153</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 1.385297</td>\n",
        "      <td> 0.002448</td>\n",
        "      <td> 1.232759</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 1.373323</td>\n",
        "      <td> 0.002658</td>\n",
        "      <td> 1.230852</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 1.402567</td>\n",
        "      <td> 0.002490</td>\n",
        "      <td> 1.225876</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 1.370034</td>\n",
        "      <td> 0.002632</td>\n",
        "      <td> 1.227810</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 1.375279</td>\n",
        "      <td> 0.002661</td>\n",
        "      <td> 1.227213</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 1.370288</td>\n",
        "      <td> 0.002647</td>\n",
        "      <td> 1.229208</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 1.374026</td>\n",
        "      <td> 0.002633</td>\n",
        "      <td> 1.228820</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 1.368643</td>\n",
        "      <td> 0.002468</td>\n",
        "      <td> 1.233158</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 1.377893</td>\n",
        "      <td> 0.002384</td>\n",
        "      <td> 1.223762</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 1.373120</td>\n",
        "      <td> 0.002643</td>\n",
        "      <td> 1.224900</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 1.402113</td>\n",
        "      <td> 0.002516</td>\n",
        "      <td> 1.225156</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 1.373776</td>\n",
        "      <td> 0.002317</td>\n",
        "      <td> 1.235618</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "           a         b         c\n",
        "0   1.387132  0.002991  1.240450\n",
        "1   1.383202  0.002530  1.225369\n",
        "2   1.374979  0.002628  1.226153\n",
        "3   1.385297  0.002448  1.232759\n",
        "4   1.373323  0.002658  1.230852\n",
        "5   1.402567  0.002490  1.225876\n",
        "6   1.370034  0.002632  1.227810\n",
        "7   1.375279  0.002661  1.227213\n",
        "8   1.370288  0.002647  1.229208\n",
        "9   1.374026  0.002633  1.228820\n",
        "10  1.368643  0.002468  1.233158\n",
        "11  1.377893  0.002384  1.223762\n",
        "12  1.373120  0.002643  1.224900\n",
        "13  1.402113  0.002516  1.225156\n",
        "14  1.373776  0.002317  1.235618"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**BONUS** Now repeat the past two problems but instead of computing the matrix product, compute a matrix's [determinant](http://en.wikipedia.org/wiki/Determinant).  Measure performance for matricies of various sizes and discuss the results.  Determinant may get impractical to calculate for not too huge of matricies, so no need to goto 1000x1000 matricies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###IO Exercises\n",
      "\n",
      "Below is a map of various datatypes in python that you have come across and their corresponding JSON equivalents.\n",
      "\n",
      "$$Datatypes=\\left[ \\begin{array}{cc} JSON & Python3 \\\\ object & dictionary \\\\ array & list \\\\ string & string \\\\ integer\t& integer \\\\ real number & float \\\\ true & True \\\\ false & False \\\\ null & None  \\end{array} \\right]$$\n",
      "\n",
      "\n",
      "There are atleast two very important python datatypes missing in the above list. \n",
      "Can you find the same?  [list the two mising python datatypes in this markdown cell below]\n",
      "\n",
      "1. **Tuple**\n",
      "2. **Set**\n",
      "\n",
      "Now We can save the above map as a dictionary with Key-value pairs \n",
      "1. create a python dictionary named dataypes, having the above map as the Key-value pairs with Python datatypes as values and JSON equivalents as keys.\n",
      "2. Save it as a pickle called datatypes and gzip the same.\n",
      "3. Reload this pickle, and read the file contents and output the data in the following formatted way as given in this example - \"The JSON equivalent for the Python datatype Dictionary is Object\". Output similarly for the rest of the key-value pairs.\n",
      "4. Save this data as a JSON but using Python datatypes as keys and JSON equivalent as values this time. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1) define python dictionary\n",
      "datatypes = {'object':'dictionary', \n",
      "             'array':'list',\n",
      "             'string':'string',\n",
      "             'integer':'integer',\n",
      "             'realnumber': 'float',\n",
      "             'true':'True',\n",
      "             'false':'False',\n",
      "             'null':'None'}\n",
      "\n",
      "#2) save dict as pickle and write to gzip file\n",
      "import pickle, gzip\n",
      "with gzip.open('datatypes.gz','wb') as zippedPickle:\n",
      "    pickle.dump(datatypes,zippedPickle)\n",
      "\n",
      "#3) read the file and print in desired formats\n",
      "with gzip.open('datatypes.gz','rb') as inFile:\n",
      "    new_datatypes = pickle.load(inFile)\n",
      "for json_dt, python_dt in new_datatypes.items():\n",
      "    print \"The JSON equivalent for the Python datatype %s is %s\" % (python_dt, json_dt)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The JSON equivalent for the Python datatype False is false\n",
        "The JSON equivalent for the Python datatype string is string\n",
        "The JSON equivalent for the Python datatype dictionary is object\n",
        "The JSON equivalent for the Python datatype integer is integer\n",
        "The JSON equivalent for the Python datatype list is array\n",
        "The JSON equivalent for the Python datatype None is null\n",
        "The JSON equivalent for the Python datatype True is true\n",
        "The JSON equivalent for the Python datatype float is realnumber\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#4)\n",
      "# interchanging the keys and values\n",
      "json_dict = {}\n",
      "for json_dt, python_dt in new_datatypes.items():\n",
      "    json_dict[python_dt] = json_dt\n",
      "\n",
      "# writing the json file\n",
      "import json\n",
      "with open('json_dict.json', 'w') as outfile:\n",
      "    json.dump(json_dict, outfile)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###SQL Exercises\n",
      "1. Create a database \"Datatype\", having a table \"datatypes\"\n",
      "2. Save the above datatype pickle in the table datatypes\n",
      "3. Create another table named \"missing_datatypes\"\n",
      "4. Add the 2 missing datatypes that you just wrote in the above markdown cell in the new sql table\n",
      "5. Write a SQL query to merge table mising_datatypes into table datatypes\n",
      "6. Write a SQL query to sort the merged table based on the values in python column and output the sorted table\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1,2)\n",
      "import sqlite3, gzip, pickle\n",
      "\n",
      "# initiate cursor in sqlite database 'Datatype.sqlite'\n",
      "con = sqlite3.connect(\"Datatype.sqlite\")\n",
      "\n",
      "# reading dict pickle from .gz file, load into dataframe\n",
      "with gzip.open('datatypes.gz','rb') as inFile:\n",
      "    dtDict = pickle.load(inFile)\n",
      "DTdf = pd.DataFrame( dtDict.items(), columns=['json_dt','python_dt'])\n",
      "\n",
      "# write dataframe to table 'datatypes'\n",
      "DTdf.to_sql('datatypes', con, flavor='sqlite', if_exists='replace')\n",
      "\n",
      "# test the table\n",
      "#dd = pd.read_sql(\"SELECT * from datatypes LIMIT 4\", con)\n",
      "#dd\n",
      "\n",
      "# 3,4)\n",
      "# create dataframe with missing datatypes, add to table\n",
      "MDTdf = pd.DataFrame(['tuple','set'], columns=['python_dt'])\n",
      "MDTdf.to_sql('missing_datatypes', con, flavor='sqlite', if_exists='replace')\n",
      "#MDTdf\n",
      "\n",
      "#5)\n",
      "# insert missing datatypes into 'datatypes'\n",
      "# ADQ: review pandas for SQL developers\n",
      "MDTdf.to_sql('datatypes', con, flavor='sqlite', if_exists='append')\n",
      "#dd = pd.read_sql(\"SELECT * from datatypes\", con)\n",
      "#dd\n",
      "\n",
      "#6)\n",
      "ALLdf = pd.read_sql(\"SELECT python_dt, json_dt FROM datatypes ORDER BY python_dt\", con)\n",
      "ALLdf\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>python_dt</th>\n",
        "      <th>json_dt</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>      False</td>\n",
        "      <td>      false</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>       None</td>\n",
        "      <td>       null</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>       True</td>\n",
        "      <td>       true</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> dictionary</td>\n",
        "      <td>     object</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>      float</td>\n",
        "      <td> realnumber</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>    integer</td>\n",
        "      <td>    integer</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>       list</td>\n",
        "      <td>      array</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>        set</td>\n",
        "      <td>       None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>     string</td>\n",
        "      <td>     string</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>      tuple</td>\n",
        "      <td>       None</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "    python_dt     json_dt\n",
        "0       False       false\n",
        "1        None        null\n",
        "2        True        true\n",
        "3  dictionary      object\n",
        "4       float  realnumber\n",
        "5     integer     integer\n",
        "6        list       array\n",
        "7         set        None\n",
        "8      string      string\n",
        "9       tuple        None"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Regular Expression\n",
      "We have a file regex_test which has number of lines, each line containing one data element of various kinds (dates, numbers, phone numbers, etc).  Your task is to construct a python array containing all instances of each of the specified data types via python regular expressions.\n",
      "\n",
      "Here are the data types:\n",
      "\n",
      "1. Dates of one of these formats: mm/dd/yyyy, yyyymmdd, yyyy-mm-dd\n",
      "2. US phone numbers: 10 digits, area code can be surounded by () or seperated by one dash and some spaces, prefix/line number can be seperated by one dash and some spaces.  Example valid numbers are 123-456-7890, (123)   456 -7890, 1234567890, 123- 4567890, etc\n",
      "3. Raw decimal numbers (IE somthing that \"float(x)\" would not crash on). \n",
      "4. US Dollar amounts (\\$ddd.cc)\n",
      "5. Basic email addresses.  Should conform to http://en.wikipedia.org/wiki/Email_address but with only aphanumeric usernames/domains, only com, co.uk, edu, and ru tlds are used\n",
      "\n",
      "Please create a list of the valid elements for each of these types and report the number of valid elements of each type in a markdown field."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "# read file, remove newlines\n",
      "with open('regex_test','r') as ff:\n",
      "    test_strings = ff.readlines()\n",
      "test_strings = [ss.replace('\\n','') for ss in test_strings]\n",
      "\n",
      "#\n",
      "#  regex searchers, along with examples they match                    EXAMPLES\n",
      "datePatterns =  [re.compile( r'^(\\d{2})/(\\d{2})/(\\d{4})$' ),          # mm/dd/yyyy\n",
      "                 re.compile( r'^(\\d{4})-(\\d{2})-([[0-3]\\d{1})$' ),    # yyyy-mm-dd\n",
      "                 re.compile( r'^(\\d{8})$' ),]                         # yyyymmdd\n",
      "phonePatterns = [re.compile( r'(^\\D*\\d{3})\\D*(\\d{3})\\D*(\\d{4})$' ) ]  # 123-456-7890 (123)456.789\n",
      "floatPatterns = [re.compile( r'^(\\.\\d+)$' ),                          # .3 .423\n",
      "                 re.compile( r'^\\d+\\.$' ),                            # 3. 244. \n",
      "                 re.compile( r'^\\d+$' )]                              # 3 244\n",
      "usdPatterns   = [re.compile( r'^\\$\\d*\\.\\d*' )]                        # $.34 $23.3 $34.\n",
      "emailPatterns = [re.compile( r'^\\w+\\@\\w+\\.(com|co\\.uk|edu|ru)' )]     # (alphanum emails only)\n",
      "\n",
      "\n",
      "# makeSearchDict\n",
      "# INPUT:\n",
      "#  stringArr  - a list of strings to be searched\n",
      "#  rePatterns - a list of the re.compile() patterns to search\n",
      "# OUTPUT: \n",
      "#  a dictionary with the following keys:\n",
      "#   'reList':    a list of rePatterns used in the search\n",
      "#   'matchList': a list of words that satisfied the re.search\n",
      "#   'nMatches:   the number of words in matchList\n",
      "def makeSearchDict( stringArr, rePatterns):\n",
      "    # initialize output dict\n",
      "    outDict = { 'reList':rePatterns, 'matchList':[], 'nMatches':0}\n",
      "    for thisString in stringArr:        # loop on strings searched\n",
      "        for thisPattern in rePatterns:    # loop over regex's\n",
      "            searchResult = thisPattern.search(thisString)\n",
      "            if searchResult:\n",
      "                outDict['matchList'].append(thisString)\n",
      "                outDict['nMatches'] += 1\n",
      "    return outDict\n",
      "\n",
      "\n",
      "# running the search and creating the dictionaries\n",
      "dateDict  = makeSearchDict( test_strings, datePatterns)\n",
      "phoneDict = makeSearchDict( test_strings, phonePatterns)\n",
      "floatDict = makeSearchDict( test_strings, floatPatterns)\n",
      "usdDict   = makeSearchDict( test_strings, usdPatterns)\n",
      "emailDict = makeSearchDict( test_strings, emailPatterns)\n",
      "\n",
      "print \"Date matches:\", dateDict['nMatches']\n",
      "print dateDict['matchList']\n",
      "\n",
      "print \"Phone matches:\", phoneDict['nMatches']\n",
      "print phoneDict['matchList']\n",
      "\n",
      "print \"Float matches:\", floatDict['nMatches']\n",
      "print floatDict['matchList']\n",
      "\n",
      "print \"Dollar matches:\", usdDict['nMatches']\n",
      "print usdDict['matchList']\n",
      "\n",
      "print \"Email matches:\", emailDict['nMatches']\n",
      "print emailDict['matchList']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Date matches: 14\n",
        "['20001040', '2013-12-02', '1980-01-15', '2000-13-00', '12341234', '31/10/2005', '12/10/1941', '04/15/1999', '20101001', '20141301', '1900-12-01', '30/10/1987', '12362142', '20001021']\n",
        "Phone matches: 5\n",
        "['322-333-4233', '(800)     500       - 2111', '522222-4421', '2091234444', '(516)-3332123']\n",
        "Float matches: 12\n",
        "['198755', '20001040', '21244421345', '12341234', '2091234444', '20101001', '.000000', '20141301', '.41234', '12362142', '20001021', '2150021']\n",
        "Dollar matches: 5\n",
        "['$00000.1', '$100.21', '$21223.11', '$.123', '$2221111.0012']\n",
        "Email matches: 6\n",
        "['bush@whitehouse.co.uk', 'famo124@jackson1.com', 'james@ca.co.uk', 'pit@pit.edu', 'putin@kremlin.ru', 'lisuk@gmail.com']\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##REGEX RESULTS:\n",
      "\n",
      "From this file and my defined regular expression searcers, these are my results.\n",
      "\n",
      "####Date matches: 14\n",
      ">['20001040', '2013-12-02', '1980-01-15', '2000-13-00', '12341234', '31/10/2005', '12/10/1941', '04/15/1999', '20101001', '20141301', '1900-12-01', '30/10/1987', '12362142', '20001021']\n",
      "\n",
      "\n",
      "####Phone matches: 5\n",
      ">['322-333-4233', '(800)     500       - 2111', '522222-4421', '2091234444', '(516)-3332123']\n",
      "\n",
      "####Float matches: 12\n",
      ">['198755', '20001040', '21244421345', '12341234', '2091234444', '20101001', '.000000', '20141301', '.41234', '12362142', '20001021', '2150021']\n",
      "\n",
      "####Dollar matches: 5\n",
      ">['\\$00000.1', '\\$100.21', '\\$21223.11', '\\$.123', '\\$2221111.0012']\n",
      "\n",
      "####Email matches: 6\n",
      ">['bush@whitehouse.co.uk', 'famo124@jackson1.com', 'james@ca.co.uk', 'pit@pit.edu', 'putin@kremlin.ru', 'lisuk@gmail.com']"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Pandas Data Analysis\n",
      "Pandas gives us a nice set of tools to work with columnar data (similar to R's dataframe). \n",
      "To learn how to use this it makes the most sense to use a real data set.\n",
      "For this assignment we'll use the KDD Cup 1998 dataset, which can be sourced from http://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html .\n",
      "\n",
      "\n",
      "###Acquiring Data\n",
      "First we pull the README file from the dataset into this notebook via the unix \"curl\" command.  Remember you can hide/minimize output cells via the button on the left of the output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/readme"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+--------------------------------------------------------------------+\r",
        "\r\n",
        "| NOTE TO ALL DOWN-LOADERS                                           |\r",
        "\r\n",
        "+--------------------------------------------------------------------+\r",
        "\r\n",
        "\r",
        "\r\n",
        "The KDD-CUP-98 data set and the accompanying documentation are now \r",
        "\r\n",
        "available for general use with the following restrictions: \r",
        "\r\n",
        "\r",
        "\r\n",
        "  (1) The users of the data must notify \r",
        "\r\n",
        "\r",
        "\r\n",
        "\tIsmail Parsa\t(iparsa@epsilon.com) and\r",
        "\r\n",
        "\tKen Howes\t(khowes@epsilon.com) \r",
        "\r\n",
        "\r",
        "\r\n",
        "  in the event they produce results, visuals or tables, etc. from the \r",
        "\r\n",
        "  data and send a note that includes a summary of the final result. \r",
        "\r\n",
        "\r",
        "\r\n",
        "  (2) The authors of published and/or unpublished articles that use \r",
        "\r\n",
        "  the KDD-Cup-98 data set must also notify the individuals listed \r",
        "\r\n",
        "  above and send a copy of their published and/or unpublished work. \r",
        "\r\n",
        "\r",
        "\r\n",
        "  (3) If you intend to use this data set for training or educational\r",
        "\r\n",
        "  purposes, you must not reveal the name of the sponsor PVA \r",
        "\r\n",
        "  (Paralyzed Veterans of America) to the trainees or students. You \r",
        "\r\n",
        "  are allowed to say \"a national veterans organization\"...\r",
        "\r\n",
        "\r",
        "\r\n",
        "\r",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "For more information regarding the KDD-Cup (including the list of the \r",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "participants and the results), please visit the KDD-Cup-98 web page at\r",
        "\r\n",
        "\r",
        "\r\n",
        "\thttp://www.epsilon.com/new\r",
        "\r\n",
        "\r",
        "\r\n",
        "While there, scroll down to Data Mining Presentations where you will \r",
        "\r\n",
        "find the KDD-Cup-98 web page. \r",
        "\r\n",
        "\r",
        "\r\n",
        "\r",
        "\r\n",
        "Ismail Parsa \r",
        "\r\n",
        "Epsilon \r",
        "\r\n",
        "50 Cambridge Street \r",
        "\r\n",
        "Burlington MA 01803 USA \r",
        "\r\n",
        "\r",
        "\r\n",
        "TEL: (781) 685-6734 \r",
        "\r\n",
        "FAX: (781) 685-0806 \r",
        "\r\n",
        "\r",
        "\r\n",
        "+--------------------------------------------------------------------+\r",
        "\r\n",
        "| LISTING of the FILES (README FILE)                                 |\r",
        "\r\n",
        "+--------------------------------------------------------------------+\r",
        "\r\n",
        "\r",
        "\r\n",
        "File Naming Conventions: \r",
        "\r\n",
        "\r",
        "\r\n",
        "o cup98               : KDD-CUP-98 \r",
        "\r\n",
        "o      QUE            : QUEstionnaire \r",
        "\r\n",
        "o      DOC            : DOCumentation\r",
        "\r\n",
        "o      DIC            : DICtionary\r",
        "\r\n",
        "o      LRN            : LeaRNing data set \r",
        "\r\n",
        "o      VAL            : VALidation data set \r",
        "\r\n",
        "o      VALtargt       : TARGeT fields for VALidation data set\r",
        "\r\n",
        "o              .txt   : plain ascii text files\r",
        "\r\n",
        "o              .zip   : PKZIP compressed files\r",
        "\r\n",
        "o              .txt.Z : UNIX COMPRESSED files\r",
        "\r\n",
        "\r",
        "\r\n",
        "FILE NAME       DESCRIPTION\r",
        "\r\n",
        "--------------- ------------------------------------------------------\r",
        "\r\n",
        "\r",
        "\r\n",
        "README          This list, listing the files in the FTP server and \r",
        "\r\n",
        "                their contents.\r",
        "\r\n",
        "\r",
        "\r\n",
        "cup98NDA.txt    The Non-Disclosure Agreement. MUST BE SIGNED BY ALL \r",
        "\r\n",
        "                PARTICIPANTS AND MAILED BACK TO ISMAIL PARSA \r",
        "\r\n",
        "                <iparsa@epsilon.com> BEFORE DOWNLOADING THE DATA SETS.\r",
        "\r\n",
        "\r",
        "\r\n",
        "cup98DOC.txt    This file, an overview and pointer to more detailed \r",
        "\r\n",
        "                information about the competition\r",
        "\r\n",
        "\r",
        "\r\n",
        "cup98DIC.txt    Data dictionary to accompany the analysis data set. \r",
        "\r\n",
        "\r",
        "\r\n",
        "cup98QUE.txt    KDD-CUP questionnaire. PARTICIPANTS ARE REQUIRED TO\r",
        "\r\n",
        "                FILL-OUT THE QUESTIONNAIRE and turned in\r",
        "\r\n",
        "                with the results. \r",
        "\r\n",
        "\r",
        "\r\n",
        "cup98LRN.zip    PKZIP compressed raw LEARNING data set. \r",
        "\r\n",
        "                Internal name: cup98LRN.txt \r",
        "\r\n",
        "                File size: 36,468,735 bytes zipped. 117,167,952 bytes \r",
        "\r\n",
        "                unzipped.\r",
        "\r\n",
        "                Number of Records: 95412.\r",
        "\r\n",
        "                Number of Fields: 481.\r",
        "\r\n",
        "\r",
        "\r\n",
        "cup98VAL.zip    PKZIP compressed raw VALIDATION data set. \r",
        "\r\n",
        "                Internal name: cup98VAL.txt \r",
        "\r\n",
        "                File size: 36,763,018 bytes zipped. 117,943,347 bytes \r",
        "\r\n",
        "                unzipped.\r",
        "\r\n",
        "                Number of Records: 96367.\r",
        "\r\n",
        "                Number of Fields: 479.\r",
        "\r\n",
        " \r",
        "\r\n",
        "cup98LRN.txt.Z  UNIX COMPRESSed raw LEARNING data set. \r",
        "\r\n",
        "                Internal name: cup98LRN.txt \r",
        "\r\n",
        "                File size: 36,579,127 bytes compressed. 117,167,952 \r",
        "\r\n",
        "                bytes uncompressed.\r",
        "\r\n",
        "                Number of Records: 95412.\r",
        "\r\n",
        "                Number of Fields: 481.\r",
        "\r\n",
        "\r",
        "\r\n",
        "cup98VAL.txt.Z  UNIX COMPRESSed raw VALIDATION data set. \r",
        "\r\n",
        "                Internal name: cup98VAL.txt \r",
        "\r\n",
        "                File size: 36,903,761 bytes compressed. 117,943,347 \r",
        "\r\n",
        "                bytes uncompressed.\r",
        "\r\n",
        "                Number of Records: 96367.\r",
        "\r\n",
        "                Number of Fields: 479.\r",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see this README describes several files which may be of use.  In particular there are two more documentation files (DOC and DIC) we should read to get an idea of the data format.  Bring these files into the notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we wish to download the cup98lrn.zip file and unzip it into a new subdirectory called \"data\".  \n",
      "However, since this file is pretty big we don't want to store it on github.  \n",
      "Luckily git provides the [.gitignore](http://git-scm.com/docs/gitignore) file which allows us to specify files we don't want to put into our git repository.\n",
      "\n",
      "Please do the following steps:\n",
      "\n",
      "1. Add the directory \"data\" to the .gitignore file\n",
      "2. Commit the new .gitignore file\n",
      "3. Create a new directory \"data\"\n",
      "4. Download http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98lrn.zip into the data directory\n",
      "5. Unzip the cup98lrn.zip (we will only be using the unzipped version, so feel free to remove the zip file)\n",
      "6. Run \"git status\" to show that the data directory is not an untracked file (this indicates it is ignored)\n",
      "\n",
      "**NOTE:** These steps only need to be run once, it is advised you comment all the lines out by putting a # at the start of each line after they have run.  This will save you time in the future when you have to rerun all cells/don't want to spend a few minutes downloading the data file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now perform some basic sanity checks on the data.  Using a combination of unix/basic python answer the following questions:\n",
      "\n",
      "1. How many lines are there?  \n",
      "2. Is the file character seperated or fixed width format?\n",
      "3. Is there a header?  If so how many fields are in it?\n",
      "4. Do all rows have the same number of fields as the header?\n",
      "5. Does anyhting in 1-4 disagree with the readme file or indicate erroneous data?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Give answers to questions 1-4 in this markdown cell:\n",
      "\n",
      "1. \n",
      "2. \n",
      "3. \n",
      "4. \n",
      "\n",
      "Now load the data file into a pandas data frame called \"learn\".  To save some time, we've loaded the data dictionary into col_types.  \n",
      "\n",
      "Finally split learn into two data frames, learn_y: the targets (two columns described in the documentation) and learn_x: the predictors (everything but the targets)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dict_file = open(\"dict.dat\")\n",
      "col_types = [ (x.split(\"\\t\")[0], x.strip().split(\"\\t\")[1]) for x in dict_file.readlines() ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Summarizing Data\n",
      "Now that we have loaded data into the learn table, we wish to to summarize the data.  \n",
      "Write a function called summary which takes a pandas data frame and prints a summary of each column containing the following:\n",
      "\n",
      "If the column is numeric:\n",
      "\n",
      "1. Mean\n",
      "2. Standard Deviation\n",
      "3. Min/Max\n",
      "4. Number of missing values (NaN, Inf, NA)\n",
      "\n",
      "If the column is non alphabetical:\n",
      "\n",
      "1. Number of distinct values\n",
      "2. Number of missing values (NaN, INF, NA, blank/all spaces)\n",
      "3. The frequency of the 3 most common values and 3 least common values\n",
      "\n",
      "Format the output to be human readable.\n",
      "\n",
      "For example:\n",
      "> Field_1  \n",
      "> mean: 50  \n",
      "> std_dev: 25  \n",
      "> min: 0  \n",
      "> max: 100  \n",
      "> missing: 5\n",
      ">  \n",
      "> Field_2  \n",
      "> distinct_values: 100  \n",
      "> missing: 10  \n",
      ">  \n",
      "> 3 most common:  \n",
      ">   the: 1000  \n",
      ">   cat: 950  \n",
      ">   meows: 900  \n",
      ">  \n",
      "> 3 least common:  \n",
      ">   dogs: 5  \n",
      ">   lizards: 4  \n",
      ">   eggs: 1  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Feature Engineering\n",
      "\n",
      "As you should have seen, there are a number of columns which are encoded as text.  To perform modeling we must convert these to somthing more numeric or throw them away.  There are a number of ways to do this which are useful in different situations.  For each of the following methods described, write a function that will take a data frame, the column you want to process, any other needed arguements and will return a data frame with the target column removed and new columns added for the described features.\n",
      "\n",
      "1. Hot 1 encoding: takes a column \"Field\" with n unique values and adds n new columns \"Field_x\" with x being each unique value of the field.  A record will have Field_x = 1 if Field = x in the original record.\n",
      "2. Risk table: If a record contains x in Field, replace x with the percent of all records containing x that are taged as positive.  IE if there are 100 records with the zip code 92110 10 of which are positive, replace 92110 in the zip field with 0.1.\n",
      "3. Regex contains: similar to hot one, but instead allow the user to specify some regex patterns x and set Field_x to 1 if the regex pattern x matches the value in Field.  An example usage might be to standardize occupations to student/employed/unemployed.\n",
      "4. Erase feature: remove the feature and don't add any features\n",
      "5. **BONUS:** One unique featureization idea of your choosing.  \n",
      "\n",
      "After this, apply one of your featurizers for each of the 75 non numeric columns.  Since there are so many columns you should use the summary statisitics to choose hot 1, risk table, or erase for most of them.  Come up with a basic heuristic to choose one of them (*Hint: you don't want to generate a ton of mostly constant features*).  Use the regex contains featureizer on at least 2 columns to extract some interesting meaning.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that learn_y contains *only* numeric columns, we wish to normalize all features so that they all have similar ranges of values.  To do this we apply the [Z-transform](http://en.wikipedia.org/wiki/Standard_score).  Apply this independently to all columns in the data set.  This is the whitening transofrm used by the PCA model in the iris notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Modeling\n",
      "\n",
      "Note that there are two labeles for this dataset: weather a person donated or not and how much they donated if they donated.  To accurately model this we would need more complicated techniques you will learn in later coursework.  However, for now we can do some exploratory analysis similar to the housing/iris notebooks we saw in class friday.\n",
      "\n",
      "Do the following steps:\n",
      "\n",
      "**Unsupervised Modeling**\n",
      "1. Use PCA to project the features onto a 2 dimensional subspace, join with the labels and plot each point using the doner/non doner to color points.  Do the labels look cleanly seperated in this 2 dimensional subspace?\n",
      "2. Cluster the raw features into 2 clusters.  Compute the percentage of donors in each cluster.  Do one of the clusters have a higher donor rate than the other?  Plot the two features which seem to seperate the clusters the most, coloring the points by the labels.\n",
      "\n",
      "**Supervised Modeling**\n",
      "1. Build a regression model on the set of known donors to predict the donation amount from all features.\n",
      "2. Compute the mean error and plot the true donation size vs predicted donation size.\n",
      "3. Print a list of features/weights for hte linear regression model sorted by absolute weight.  Do the highest weight features seem like things that would affect donation size?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}