{
 "metadata": {
  "name": "",
  "signature": "sha256:bb95b7d06b79cf87aa2759c43bb0107901db688df7e1c2b91ccbbd5d1a10b1ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import the libraries \n",
      "%pylab inline\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import pandas as pd\n",
      "import sklearn.linear_model\n",
      "from sklearn.decomposition import PCA\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Object Oriented Programming for Analysis\n",
      "\n",
      "Last week we explored feature engineering and binary classification on the DCT data set.  This was all well and good but due to the ad hoc nature of this it may be hard to reuse this code for analysis of a similar dataset.  In this notebook we guide you thought the creation of a set of classes to encapselate this code for reuseability.\n",
      "\n",
      "**The goal of this is to mostly reuse your code from the DCT analysis notebook.  In the process it is also a good chance to refactor your code**\n",
      "\n",
      "##Model Object\n",
      "\n",
      "As a first step, we wish to encapselate the training of a logistic regression training and model which uses a subset of the features in a class.  Below we have written a template for the LR_Model class which trains a model in the constructor and then encapselates the prediction/accuracy measuring code within the class.  Fill in the 3 methods.\n",
      "\n",
      "If you need to add any additional methods for use within the class, Python style is to give them names starting with an underscore.  For example if we needed an internal method to compute the accuracy it would be called \\_accuracy(p,y)\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load data for testing purposes\n",
      "train_x = pd.read_csv(\"../lecture_3_pandas_models_pylab/data/train\", sep=\"\\t\")\n",
      "train_y = train_x[\"label\"]\n",
      "del train_x[\"label\"]\n",
      "test_x = pd.read_csv(\"../lecture_3_pandas_models_pylab/data/test\", sep=\"\\t\")\n",
      "test_y = test_x[\"label\"]\n",
      "del test_x[\"label\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LR_Model:\n",
      "    def __init__(self, train_x, train_y, fields = None):\n",
      "        \"\"\"\n",
      "            Trains a logistic regression model using sklearn\n",
      "            \n",
      "            :param train_x the pandas data frame containing the features for the training data\n",
      "            :param train_y the pandas data frame containing the lables for the training data\n",
      "            :param fields  the list of features from train_x that should be used for training.  \n",
      "                           if None then train on all features       \n",
      "        \"\"\"\n",
      "        self.model = sklearn.linear_model.LogisticRegression()\n",
      "        if fields == None:\n",
      "            self.fields = train_x.columns.tolist()\n",
      "        else:\n",
      "            self.fields = fields\n",
      "\n",
      "        self.model.fit( train_x[self.fields], train_y )\n",
      "            \n",
      "    def predict(self, x):\n",
      "        \"\"\" Scores x using the logistic regression model and returns the series of preditions \"\"\"\n",
      "        return self.model.predict(x[self.fields])\n",
      "        \n",
      "    def evaluate(self, x,y):\n",
      "        \"\"\" Scores x using the logistic regression model and returns the accuracy of the scores vs y \"\"\"\n",
      "        return self.model.score(x[self.fields],y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After this code is complete the following tests should return true"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = LR_Model(train_x,train_y)\n",
      "print model.evaluate(test_x,test_y) > .90\n",
      "print model.evaluate(test_x, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "0.907949790795\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = LR_Model( train_x, train_y, test_x.columns.tolist()[0:10] )\n",
      "print model.evaluate(test_x,test_y) > 0.55\n",
      "print model.evaluate(test_x, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "0.560669456067\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Feature Engineering Class\n",
      "\n",
      "We now provide you with a class FE_Technique which exports single method *transform* which takes a data frame and returns a new data frame containing the original frame plus the features created by the specific feature engineering technique.  **Do not modify this code.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FE_Technique:\n",
      "    def __init__(self, train_x):\n",
      "        \"\"\"\n",
      "            Constructs the FE_Technique using training data.  The constructor can do \n",
      "            any prepetory work if the technique is data specific.\n",
      "        \"\"\"\n",
      "        self.pass_through = True\n",
      "        None\n",
      "        \n",
      "    def _technique(self, data_frame):\n",
      "        \"\"\"\n",
      "            A private abstract method which takes the data frame and creates a new data frame \n",
      "            containin ONLY the features generated by the specific feature engineering technique.\n",
      "            This method is overridden in specific techniques.\n",
      "        \"\"\"\n",
      "        raise (\"This method is abstract\")\n",
      "        \n",
      "    def transform(self, data_frame):\n",
      "        \"\"\"\n",
      "            Applies the feature engineering technique and returns a new data frame contaning the\n",
      "            input features plus the new features.  This should not be overridden in subclasses\n",
      "        \"\"\"\n",
      "        added_features = self._technique(data_frame)\n",
      "        if self.pass_through:\n",
      "            return pd.concat([data_frame, added_features], axis=1, ignore_index = False)\n",
      "        else:\n",
      "            return added_features\n",
      "        \n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now impliment these 3 techniques as subclasses fo the FE_Technique class:\n",
      "\n",
      "1. FE_Basic_Math - impliments the absolute value, sign, and squareing transforms seen in last weeks homework\n",
      "2. FE_Whiten - impliments the Whitening Z Transform we've seen the past two homeworks.  This should set self.pass_through to false so that only the whitened features are output.  *Hint: in later parts we may see data with standard deviations of 0 for certain columns, account for this case*\n",
      "3. FE_PCA - add the top **20** PCA coeficient features to the data frame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FE_Basic_Math(FE_Technique):\n",
      "    def _technique(self, data_frame):\n",
      "        \"\"\"Applies the sign, abs, and square transforms to all fields in data_frame\"\"\"\n",
      "        mathOnlyDF = pd.DataFrame()\n",
      "        for col in data_frame.columns:\n",
      "            mathOnlyDF[str(col) + \"_SQ\"]   = data_frame[col]**2\n",
      "            mathOnlyDF[str(col) + \"_ABS\"]  = abs(data_frame[col])\n",
      "            mathOnlyDF[str(col) + \"_SIGN\"] = np.sign(data_frame[col])\n",
      "        return mathOnlyDF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FE_Whiten(FE_Technique):\n",
      "    def __init__(self,train_x):\n",
      "        #Call the super class constructor\n",
      "        FE_Technique.__init__(self, train_x)\n",
      "        self.pass_through = False\n",
      "    \n",
      "    def _technique(self, data_frame):\n",
      "        \"\"\"Whitens the input data_frame \"\"\"\n",
      "        for thisCol in data_frame.columns:\n",
      "            thisAvg = data_frame[thisCol].mean()\n",
      "            thisStd = data_frame[thisCol].std()\n",
      "            # the business\n",
      "            data_frame[thisCol] = (data_frame[thisCol]-thisAvg) / thisStd\n",
      "        return data_frame"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Write FE_PCA HERE\n",
      "class FE_PCA(FE_Technique):\n",
      "    def _technique(self, data_frame):\n",
      "        \"\"\"Returns the top 20 PCA cefficient features for data_frame\"\"\"\n",
      "        nComponents = 20  # could be made an arg\n",
      "        pca = PCA(n_components=nComponents, whiten=True)\n",
      "        pca.fit(data_frame)\n",
      "        coeffsList = pca.transform(data_frame).T\n",
      "        #put array into dataframe\n",
      "        topPCA_DF = pd.DataFrame()\n",
      "        pcaIndxArr = ['PCA_'+str(ii) for ii in range(nComponents)]\n",
      "        for colName, coeffList in zip(pcaIndxArr, coeffsList):\n",
      "            topPCA_DF[colName] = coeffList\n",
      "        return topPCA_DF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After this code is complete the following code should run and produce a reasonable data frame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Example usage\n",
      "bm = FE_Basic_Math(train_x)\n",
      "train_x_bm = bm.transform(train_x)\n",
      "\n",
      "wht = FE_Whiten(train_x_bm)\n",
      "\n",
      "train_x_bm_pca = wht.transform(train_x_bm)\n",
      "pca = FE_PCA(train_x_bm_pca)\n",
      "\n",
      "x = pca.transform(wht.transform(bm.transform(test_x)))\n",
      "x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>PCA_10</th>\n",
        "      <th>PCA_11</th>\n",
        "      <th>PCA_12</th>\n",
        "      <th>PCA_13</th>\n",
        "      <th>PCA_14</th>\n",
        "      <th>PCA_15</th>\n",
        "      <th>PCA_16</th>\n",
        "      <th>PCA_17</th>\n",
        "      <th>PCA_18</th>\n",
        "      <th>PCA_19</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td>...</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "      <td> 2.390000e+02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> 4.831096e-17</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td> 1.486491e-17</td>\n",
        "      <td> 2.601359e-17</td>\n",
        "      <td> 2.229737e-17</td>\n",
        "      <td>-7.432455e-18</td>\n",
        "      <td>-1.486491e-17</td>\n",
        "      <td>-1.486491e-17</td>\n",
        "      <td> 1.486491e-17</td>\n",
        "      <td> 1.486491e-17</td>\n",
        "      <td>...</td>\n",
        "      <td> 1.300680e-17</td>\n",
        "      <td> 1.951020e-17</td>\n",
        "      <td> 4.738190e-17</td>\n",
        "      <td> 5.574342e-17</td>\n",
        "      <td>-2.972982e-17</td>\n",
        "      <td> 1.114868e-17</td>\n",
        "      <td>-2.264576e-17</td>\n",
        "      <td>-9.290569e-18</td>\n",
        "      <td> 2.415548e-17</td>\n",
        "      <td> 1.858114e-17</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>...</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "      <td> 1.002099e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>-1.119748e+00</td>\n",
        "      <td>  -3.157541</td>\n",
        "      <td>-3.277829e+00</td>\n",
        "      <td>-4.901732e+00</td>\n",
        "      <td>-3.923467e+00</td>\n",
        "      <td>-6.132471e+00</td>\n",
        "      <td>-4.386212e+00</td>\n",
        "      <td>-5.912378e+00</td>\n",
        "      <td>-4.579980e+00</td>\n",
        "      <td>-4.571009e+00</td>\n",
        "      <td>...</td>\n",
        "      <td>-3.765547e+00</td>\n",
        "      <td>-3.836709e+00</td>\n",
        "      <td>-3.854911e+00</td>\n",
        "      <td>-5.016538e+00</td>\n",
        "      <td>-3.212136e+00</td>\n",
        "      <td>-4.777441e+00</td>\n",
        "      <td>-5.650861e+00</td>\n",
        "      <td>-4.068019e+00</td>\n",
        "      <td>-5.178029e+00</td>\n",
        "      <td>-3.316702e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>-5.853191e-01</td>\n",
        "      <td>  -0.493910</td>\n",
        "      <td>-2.915476e-01</td>\n",
        "      <td>-2.396526e-01</td>\n",
        "      <td>-4.182991e-01</td>\n",
        "      <td>-3.826004e-01</td>\n",
        "      <td>-3.072055e-01</td>\n",
        "      <td>-2.588883e-01</td>\n",
        "      <td>-5.422994e-01</td>\n",
        "      <td>-3.779934e-01</td>\n",
        "      <td>...</td>\n",
        "      <td>-6.349173e-01</td>\n",
        "      <td>-4.986733e-01</td>\n",
        "      <td>-6.429835e-01</td>\n",
        "      <td>-4.110546e-01</td>\n",
        "      <td>-6.262585e-01</td>\n",
        "      <td>-6.186984e-01</td>\n",
        "      <td>-5.822696e-01</td>\n",
        "      <td>-6.166579e-01</td>\n",
        "      <td>-5.106234e-01</td>\n",
        "      <td>-5.129546e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>-3.457477e-01</td>\n",
        "      <td>  -0.299010</td>\n",
        "      <td>-1.493438e-01</td>\n",
        "      <td>-2.396526e-01</td>\n",
        "      <td>-1.135019e-01</td>\n",
        "      <td>-1.164098e-02</td>\n",
        "      <td>-1.032552e-01</td>\n",
        "      <td> 5.519446e-02</td>\n",
        "      <td>-1.385313e-01</td>\n",
        "      <td> 3.189818e-03</td>\n",
        "      <td>...</td>\n",
        "      <td>-5.574544e-02</td>\n",
        "      <td> 8.346377e-02</td>\n",
        "      <td>-7.070840e-03</td>\n",
        "      <td>-8.186515e-02</td>\n",
        "      <td>-7.509972e-02</td>\n",
        "      <td> 1.459160e-01</td>\n",
        "      <td>-8.747083e-03</td>\n",
        "      <td> 7.951971e-02</td>\n",
        "      <td> 2.452247e-02</td>\n",
        "      <td> 1.045807e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 1.426094e-01</td>\n",
        "      <td>  -0.039143</td>\n",
        "      <td>-7.139942e-03</td>\n",
        "      <td> 9.335305e-02</td>\n",
        "      <td> 1.912953e-01</td>\n",
        "      <td> 3.593184e-01</td>\n",
        "      <td> 1.006951e-01</td>\n",
        "      <td> 3.692772e-01</td>\n",
        "      <td> 2.652368e-01</td>\n",
        "      <td> 3.843730e-01</td>\n",
        "      <td>...</td>\n",
        "      <td> 6.086292e-01</td>\n",
        "      <td> 5.728127e-01</td>\n",
        "      <td> 6.053456e-01</td>\n",
        "      <td> 4.908175e-01</td>\n",
        "      <td> 5.631774e-01</td>\n",
        "      <td> 6.424385e-01</td>\n",
        "      <td> 5.588137e-01</td>\n",
        "      <td> 5.837077e-01</td>\n",
        "      <td> 6.017896e-01</td>\n",
        "      <td> 5.861905e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 4.132395e+00</td>\n",
        "      <td>   4.053755</td>\n",
        "      <td> 4.116772e+00</td>\n",
        "      <td> 3.756415e+00</td>\n",
        "      <td> 5.220449e+00</td>\n",
        "      <td> 4.068912e+00</td>\n",
        "      <td> 5.811304e+00</td>\n",
        "      <td> 5.708684e+00</td>\n",
        "      <td> 3.899149e+00</td>\n",
        "      <td> 6.483304e+00</td>\n",
        "      <td>...</td>\n",
        "      <td> 3.274514e+00</td>\n",
        "      <td> 4.599421e+00</td>\n",
        "      <td> 3.393182e+00</td>\n",
        "      <td> 6.953183e+00</td>\n",
        "      <td> 4.141357e+00</td>\n",
        "      <td> 2.444406e+00</td>\n",
        "      <td> 3.909083e+00</td>\n",
        "      <td> 2.914379e+00</td>\n",
        "      <td> 4.135015e+00</td>\n",
        "      <td> 5.710085e+00</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 3220 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "                F_0         F_1           F_2           F_3           F_4  \\\n",
        "count  2.390000e+02  239.000000  2.390000e+02  2.390000e+02  2.390000e+02   \n",
        "mean   4.831096e-17    0.000000  1.486491e-17  2.601359e-17  2.229737e-17   \n",
        "std    1.000000e+00    1.000000  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min   -1.119748e+00   -3.157541 -3.277829e+00 -4.901732e+00 -3.923467e+00   \n",
        "25%   -5.853191e-01   -0.493910 -2.915476e-01 -2.396526e-01 -4.182991e-01   \n",
        "50%   -3.457477e-01   -0.299010 -1.493438e-01 -2.396526e-01 -1.135019e-01   \n",
        "75%    1.426094e-01   -0.039143 -7.139942e-03  9.335305e-02  1.912953e-01   \n",
        "max    4.132395e+00    4.053755  4.116772e+00  3.756415e+00  5.220449e+00   \n",
        "\n",
        "                F_5           F_6           F_7           F_8           F_9  \\\n",
        "count  2.390000e+02  2.390000e+02  2.390000e+02  2.390000e+02  2.390000e+02   \n",
        "mean  -7.432455e-18 -1.486491e-17 -1.486491e-17  1.486491e-17  1.486491e-17   \n",
        "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min   -6.132471e+00 -4.386212e+00 -5.912378e+00 -4.579980e+00 -4.571009e+00   \n",
        "25%   -3.826004e-01 -3.072055e-01 -2.588883e-01 -5.422994e-01 -3.779934e-01   \n",
        "50%   -1.164098e-02 -1.032552e-01  5.519446e-02 -1.385313e-01  3.189818e-03   \n",
        "75%    3.593184e-01  1.006951e-01  3.692772e-01  2.652368e-01  3.843730e-01   \n",
        "max    4.068912e+00  5.811304e+00  5.708684e+00  3.899149e+00  6.483304e+00   \n",
        "\n",
        "           ...             PCA_10        PCA_11        PCA_12        PCA_13  \\\n",
        "count      ...       2.390000e+02  2.390000e+02  2.390000e+02  2.390000e+02   \n",
        "mean       ...       1.300680e-17  1.951020e-17  4.738190e-17  5.574342e-17   \n",
        "std        ...       1.002099e+00  1.002099e+00  1.002099e+00  1.002099e+00   \n",
        "min        ...      -3.765547e+00 -3.836709e+00 -3.854911e+00 -5.016538e+00   \n",
        "25%        ...      -6.349173e-01 -4.986733e-01 -6.429835e-01 -4.110546e-01   \n",
        "50%        ...      -5.574544e-02  8.346377e-02 -7.070840e-03 -8.186515e-02   \n",
        "75%        ...       6.086292e-01  5.728127e-01  6.053456e-01  4.908175e-01   \n",
        "max        ...       3.274514e+00  4.599421e+00  3.393182e+00  6.953183e+00   \n",
        "\n",
        "             PCA_14        PCA_15        PCA_16        PCA_17        PCA_18  \\\n",
        "count  2.390000e+02  2.390000e+02  2.390000e+02  2.390000e+02  2.390000e+02   \n",
        "mean  -2.972982e-17  1.114868e-17 -2.264576e-17 -9.290569e-18  2.415548e-17   \n",
        "std    1.002099e+00  1.002099e+00  1.002099e+00  1.002099e+00  1.002099e+00   \n",
        "min   -3.212136e+00 -4.777441e+00 -5.650861e+00 -4.068019e+00 -5.178029e+00   \n",
        "25%   -6.262585e-01 -6.186984e-01 -5.822696e-01 -6.166579e-01 -5.106234e-01   \n",
        "50%   -7.509972e-02  1.459160e-01 -8.747083e-03  7.951971e-02  2.452247e-02   \n",
        "75%    5.631774e-01  6.424385e-01  5.588137e-01  5.837077e-01  6.017896e-01   \n",
        "max    4.141357e+00  2.444406e+00  3.909083e+00  2.914379e+00  4.135015e+00   \n",
        "\n",
        "             PCA_19  \n",
        "count  2.390000e+02  \n",
        "mean   1.858114e-17  \n",
        "std    1.002099e+00  \n",
        "min   -3.316702e+00  \n",
        "25%   -5.129546e-01  \n",
        "50%    1.045807e-02  \n",
        "75%    5.861905e-01  \n",
        "max    5.710085e+00  \n",
        "\n",
        "[8 rows x 3220 columns]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Feature Selection \n",
      "\n",
      "Now develop the Model_Learner class, on construction it takes a paramter max_features and exports a method learn which uses the given training/testing data to perform forward selection in the same manner as your last assignment.\n",
      "\n",
      "Use the two private methods `_find_next_feature` and `_forward_selection` to hold most of the specific logic, learn simply return the `_forward_slection` method (as it is currenlty defined)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Model_Learner:\n",
      "    def __init__(self, max_features = 20 ):\n",
      "        self.max_features = max_features\n",
      "        self.featList = []      # list of best features\n",
      "        self.accList  = []      # list of accuracies\n",
      "        self.model = sklearn.linear_model.LogisticRegression() # instantiate model\n",
      "        \n",
      "    def learn(self, train_x, train_y, test_x, test_y):\n",
      "        \"\"\"\n",
      "            Runs forward selection until self.max_features features are added.  \n",
      "            \n",
      "            :returns a list of tuples [(model, feature added, accuracy)] \n",
      "            containing the forward selection results\n",
      "        \"\"\"\n",
      "        return self._forward_selection(train_x, train_y, test_x, test_y)\n",
      "        \n",
      "        \n",
      "    def _find_next_feature(self, features, train_x, train_y, test_x, test_y):\n",
      "\n",
      "        bestScore = -1.0\n",
      "\n",
      "        for thisCol in train_x.columns:\n",
      "            if thisCol in features:      # skip features if already acquired\n",
      "                continue\n",
      "            tmpFeats = features + [thisCol]\n",
      "            thisModel = LR_Model(train_x,train_y, tmpFeats)\n",
      "            thisScore = thisModel.evaluate(test_x,test_y)\n",
      "\n",
      "            if thisScore > bestScore:\n",
      "                bestCol = thisCol\n",
      "                bestModel = thisModel\n",
      "                bestScore = thisScore\n",
      "                                 \n",
      "        return [bestModel, bestCol, bestScore]\n",
      "                \n",
      "\n",
      "    def _forward_selection(self, train_x, train_y, test_x, test_y):\n",
      "        outList = []\n",
      "        while len(self.featList) < self.max_features:\n",
      "            selModel, selColumn, selScore = self._find_next_feature( self.featList, \n",
      "                                                                     train_x, train_y,\n",
      "                                                                     test_x, test_y)\n",
      "            print len( self.featList), selColumn, selScore\n",
      "            self.featList.append(selColumn)\n",
      "            outList.append( (selModel, selColumn, selScore) )\n",
      "        return outList\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Modeling\n",
      "\n",
      "Now use your class structure to complete the following modeling tasks\n",
      "\n",
      "###DCT Analysis\n",
      "Load the DCT train/test data used in last week's assignment, apply the FE_Basic_Math, FE_PCA, and FE_Whiten transformers and then use the model learner to use forward selection.  List the 20 features added (in order of addition) and plot the accuracy vs number of features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load data for testing purposes\n",
      "train_x = pd.read_csv(\"../lecture_3_pandas_models_pylab/data/train\", sep=\"\\t\")\n",
      "train_y = train_x[\"label\"]\n",
      "del train_x[\"label\"]\n",
      "test_x = pd.read_csv(\"../lecture_3_pandas_models_pylab/data/test\", sep=\"\\t\")\n",
      "test_y = test_x[\"label\"]\n",
      "del test_x[\"label\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Feature engineering:\n",
      "# Now applying FE_Whiten, FE_Basic_Math, and FE_PCA (respectively)\n",
      "\n",
      "# Applying to train_x\n",
      "train_x_W = FE_Whiten(train_x).transform(train_x)\n",
      "train_x_W_BM = FE_Basic_Math(train_x_W).transform(train_x_W)\n",
      "train_x_W_BM_PCA = FE_PCA(train_x_W_BM).transform(train_x_W_BM)\n",
      "\n",
      "# Applying to test_x\n",
      "test_x_W = FE_Whiten(test_x).transform(test_x)                                  \n",
      "test_x_W_BM = FE_Basic_Math(test_x_W).transform(test_x_W)                       \n",
      "test_x_W_BM_PCA = FE_PCA(test_x_W_BM).transform(test_x_W_BM) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This will print the 20 features added\n",
      "DCT_Model = Model_Learner()\n",
      "DCT_Result = DCT_Model.learn(train_x_W_BM_PCA, train_y, test_x_W_BM_PCA, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 F_240 0.761506276151\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_416 0.790794979079\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_466 0.836820083682\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_586 0.853556485356\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_368_SQ 0.874476987448\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_396_SIGN 0.903765690377\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_778 0.907949790795\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_86 0.92050209205\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_253 0.924686192469\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_185 0.928870292887\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_60_SQ 0.933054393305\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_2 0.933054393305\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_3 0.933054393305\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_271_SQ 0.937238493724\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_7 0.937238493724\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_9 0.937238493724\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_6 0.937238493724\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_20 0.937238493724\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_15 0.937238493724\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " F_22 0.937238493724\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plotting\n",
      "nFeats = range(1,21)\n",
      "accList = [kk for ii,jj,kk in DCT_Result]\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(nFeats, accList)\n",
      "plt.title('% Accuracy by Number of Features')\n",
      "plt.xlabel('Number of Features')\n",
      "plt.ylabel('% Accuracy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "<matplotlib.text.Text at 0x10b678490>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEZCAYAAACJjGL9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10c73aed0>"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Cover Type Analaysis\n",
      "We have created a sample of 10,000 data points from the UCI cover type data set.  The goal of this part of the assignment is to do a similar analysis as the DCT using your new class structure.  The data set is in SVM Light format, a very common format for binary classification.  Since we don't want to bog you down on the loading of the data we load the data for you here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_svmlight_file\n",
      "def read_svm_light(file):\n",
      "    X, y = load_svmlight_file(file) \n",
      "    return pd.DataFrame(X.todense()), pd.DataFrame(y-1)\n",
      "X, y = read_svm_light(\"cov_data\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we have only loaded a single data frame. We have not provided a test/train split for you, you will perform this step in a later step.\n",
      "\n",
      "Before doing this, first use describe to analyze the X and the y data frames to look for any descrepencies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "      <th>...</th>\n",
        "      <th>44</th>\n",
        "      <th>45</th>\n",
        "      <th>46</th>\n",
        "      <th>47</th>\n",
        "      <th>48</th>\n",
        "      <th>49</th>\n",
        "      <th>50</th>\n",
        "      <th>51</th>\n",
        "      <th>52</th>\n",
        "      <th>53</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.0000</td>\n",
        "      <td> 10000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "      <td> 10000.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>  2863.751600</td>\n",
        "      <td>   137.871400</td>\n",
        "      <td>    11.705800</td>\n",
        "      <td>   262.161600</td>\n",
        "      <td>    35.545900</td>\n",
        "      <td>  3361.233000</td>\n",
        "      <td>   218.263700</td>\n",
        "      <td>   225.581100</td>\n",
        "      <td>   139.455900</td>\n",
        "      <td>  3607.288700</td>\n",
        "      <td>...</td>\n",
        "      <td>     0.002900</td>\n",
        "      <td>     0.006500</td>\n",
        "      <td>     0.006900</td>\n",
        "      <td>     0.000400</td>\n",
        "      <td>     0.001000</td>\n",
        "      <td>     0.0001</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.007900</td>\n",
        "      <td>     0.009300</td>\n",
        "      <td>     0.007000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>   228.042275</td>\n",
        "      <td>   103.833029</td>\n",
        "      <td>     6.483259</td>\n",
        "      <td>   202.912431</td>\n",
        "      <td>    42.598291</td>\n",
        "      <td>  1767.184917</td>\n",
        "      <td>    20.755863</td>\n",
        "      <td>    16.457931</td>\n",
        "      <td>    30.892661</td>\n",
        "      <td>  1775.401234</td>\n",
        "      <td>...</td>\n",
        "      <td>     0.053776</td>\n",
        "      <td>     0.080364</td>\n",
        "      <td>     0.082783</td>\n",
        "      <td>     0.019997</td>\n",
        "      <td>     0.031609</td>\n",
        "      <td>     0.0100</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.088535</td>\n",
        "      <td>     0.095992</td>\n",
        "      <td>     0.083377</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>  1898.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>  -108.000000</td>\n",
        "      <td>    30.000000</td>\n",
        "      <td>    85.000000</td>\n",
        "      <td>   110.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>    30.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.0000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>  2726.000000</td>\n",
        "      <td>    54.000000</td>\n",
        "      <td>     7.000000</td>\n",
        "      <td>   108.000000</td>\n",
        "      <td>     7.000000</td>\n",
        "      <td>  1777.000000</td>\n",
        "      <td>   208.000000</td>\n",
        "      <td>   217.000000</td>\n",
        "      <td>   121.000000</td>\n",
        "      <td>  2166.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.0000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>  2906.000000</td>\n",
        "      <td>   109.000000</td>\n",
        "      <td>    11.000000</td>\n",
        "      <td>   216.000000</td>\n",
        "      <td>    23.000000</td>\n",
        "      <td>  3459.500000</td>\n",
        "      <td>   222.000000</td>\n",
        "      <td>   228.000000</td>\n",
        "      <td>   139.000000</td>\n",
        "      <td>  3649.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.0000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>  3001.000000</td>\n",
        "      <td>   204.250000</td>\n",
        "      <td>    15.000000</td>\n",
        "      <td>   379.000000</td>\n",
        "      <td>    53.000000</td>\n",
        "      <td>  4824.000000</td>\n",
        "      <td>   232.000000</td>\n",
        "      <td>   237.000000</td>\n",
        "      <td>   159.000000</td>\n",
        "      <td>  5102.250000</td>\n",
        "      <td>...</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.0000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>  3846.000000</td>\n",
        "      <td>   359.000000</td>\n",
        "      <td>    57.000000</td>\n",
        "      <td>  1165.000000</td>\n",
        "      <td>   547.000000</td>\n",
        "      <td>  7023.000000</td>\n",
        "      <td>   254.000000</td>\n",
        "      <td>   254.000000</td>\n",
        "      <td>   248.000000</td>\n",
        "      <td>  7172.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.0000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 54 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "                 0             1             2             3             4   \\\n",
        "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
        "mean    2863.751600    137.871400     11.705800    262.161600     35.545900   \n",
        "std      228.042275    103.833029      6.483259    202.912431     42.598291   \n",
        "min     1898.000000      0.000000      0.000000      0.000000   -108.000000   \n",
        "25%     2726.000000     54.000000      7.000000    108.000000      7.000000   \n",
        "50%     2906.000000    109.000000     11.000000    216.000000     23.000000   \n",
        "75%     3001.000000    204.250000     15.000000    379.000000     53.000000   \n",
        "max     3846.000000    359.000000     57.000000   1165.000000    547.000000   \n",
        "\n",
        "                 5             6             7             8             9   \\\n",
        "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
        "mean    3361.233000    218.263700    225.581100    139.455900   3607.288700   \n",
        "std     1767.184917     20.755863     16.457931     30.892661   1775.401234   \n",
        "min       30.000000     85.000000    110.000000      0.000000     30.000000   \n",
        "25%     1777.000000    208.000000    217.000000    121.000000   2166.000000   \n",
        "50%     3459.500000    222.000000    228.000000    139.000000   3649.000000   \n",
        "75%     4824.000000    232.000000    237.000000    159.000000   5102.250000   \n",
        "max     7023.000000    254.000000    254.000000    248.000000   7172.000000   \n",
        "\n",
        "           ...                 44            45            46            47  \\\n",
        "count      ...       10000.000000  10000.000000  10000.000000  10000.000000   \n",
        "mean       ...           0.002900      0.006500      0.006900      0.000400   \n",
        "std        ...           0.053776      0.080364      0.082783      0.019997   \n",
        "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
        "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
        "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
        "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
        "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
        "\n",
        "                 48          49     50            51            52  \\\n",
        "count  10000.000000  10000.0000  10000  10000.000000  10000.000000   \n",
        "mean       0.001000      0.0001      0      0.007900      0.009300   \n",
        "std        0.031609      0.0100      0      0.088535      0.095992   \n",
        "min        0.000000      0.0000      0      0.000000      0.000000   \n",
        "25%        0.000000      0.0000      0      0.000000      0.000000   \n",
        "50%        0.000000      0.0000      0      0.000000      0.000000   \n",
        "75%        0.000000      0.0000      0      0.000000      0.000000   \n",
        "max        1.000000      1.0000      0      1.000000      1.000000   \n",
        "\n",
        "                 53  \n",
        "count  10000.000000  \n",
        "mean       0.007000  \n",
        "std        0.083377  \n",
        "min        0.000000  \n",
        "25%        0.000000  \n",
        "50%        0.000000  \n",
        "75%        0.000000  \n",
        "max        1.000000  \n",
        "\n",
        "[8 rows x 54 columns]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 10000.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>     0.669300</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>     0.470489</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>     0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>     0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>     1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>     1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>     1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "                  0\n",
        "count  10000.000000\n",
        "mean       0.669300\n",
        "std        0.470489\n",
        "min        0.000000\n",
        "25%        0.000000\n",
        "50%        1.000000\n",
        "75%        1.000000\n",
        "max        1.000000"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# percent of y=1 vs y=0\n",
      "float(len(y[y[0]!=0]))/len(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "0.6693"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Does anything stand out to you about this data?  In particular does it need to be whitened and are the classes balanced?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**I think these data need to be whitened because of the large differences in ranges and scales in the data.  This makes sense since the data contains very different units: length (elevation, distance to a point of interest, etc..), degrees (azimuth), and various numeric representations of physical properties (hillshade index, soil type, etc...)**\n",
      "\n",
      "**I notice that at least one of the columns (50) has no standard deviation.  After whitening, this will require that I replace the NaN values with zero via df.fillna().**\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Test Train Split\n",
      "\n",
      "Now that we have loaded the data we wish to split the data 2 ways.  We wish to randomly split this dataset so that 70% will be used for training and 30% will be used for testing (this is a pretty common splitting method).  Split your X/y data frames into train_x/test_x and train_y/test_y.  Ensure that the alignment of the x rows and y rows stays the same.  *Hint: when you subset a data frame it may have a index which has missing values, this may cause problems with your feature engineering code.  Use df.reset_index(drop=True) to make sure all data frames have a normal index.*\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I to do the split, I shuffle the row index then create the train/test\n",
      "np.random.seed(232)\n",
      "rowIndx = X.index.tolist()\n",
      "np.random.shuffle(rowIndx) # shuffled row index\n",
      "splitCut = int (len(rowIndx)*0.7)\n",
      "\n",
      "trainIndx = rowIndx[:splitCut]  # first 70% of shuffled index\n",
      "testIndx  = rowIndx[splitCut:]  # last 30%\n",
      "\n",
      "train_xx = X.iloc[trainIndx].reset_index(drop=True).astype(float)\n",
      "train_yy = y.iloc[trainIndx].reset_index(drop=True).astype(float)\n",
      "\n",
      "test_xx  = X.iloc[testIndx].reset_index(drop=True).astype(float)\n",
      "test_yy  = y.iloc[testIndx].reset_index(drop=True).astype(float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Feature engineering: doing the same as above\n",
      "\n",
      "# Applying to train_x\n",
      "train_xx_W = FE_Whiten(train_xx).transform(train_xx)\n",
      "train_xx_W.fillna(value=0.0,inplace=True)              # replace NaN with zero\n",
      "train_xx_W_BM = FE_Basic_Math(train_xx_W).transform(train_xx_W)\n",
      "train_xx_W_BM_PCA = FE_PCA(train_xx_W_BM).transform(train_xx_W_BM)\n",
      "\n",
      "# Applying to test_x\n",
      "test_xx_W = FE_Whiten(test_xx).transform(test_xx)  \n",
      "test_xx_W.fillna(value=0.0,inplace=True)               # replace NaN with zero\n",
      "test_xx_W_BM = FE_Basic_Math(test_xx_W).transform(test_xx_W)                       \n",
      "test_xx_W_BM_PCA = FE_PCA(test_xx_W_BM).transform(test_xx_W_BM) \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Modeling\n",
      "\n",
      "Now apply your feature engineering techniques and use forward selection to train models with up to 20 features.  Plot the accuracy vs feature count as you did before."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This will print the 20 features added\n",
      "# (this is the same as above)\n",
      "DCT_Model = Model_Learner()\n",
      "DCT_Result = DCT_Model.learn(train_x_W_BM_PCA, train_yy[0], test_x_W_BM_PCA, test_yy[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 10 0.763666666667\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 0.825\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.844333333333\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.850666666667\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 45_SQ 0.853333333333\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 36_SQ 0.855333333333\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25_SQ 0.859\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24_SQ 0.859333333333\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.86\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " PCA_3 0.860333333333\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 22 0.860333333333\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24 0.860333333333\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 28 0.860333333333\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 34 0.860333333333\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 38 0.860333333333\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 40 0.860333333333\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 41 0.860333333333\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 49 0.860333333333\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50 0.860333333333\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11_SQ 0.860333333333\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plotting\n",
      "nFeats = range(1,21)\n",
      "accList = [kk for ii,jj,kk in DCT_Result]\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(nFeats, accList)\n",
      "plt.title('% Accuracy by Number of Features')\n",
      "plt.xlabel('Number of Features')\n",
      "plt.ylabel('% Accuracy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "<matplotlib.text.Text at 0x106b85110>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEZCAYAAACJjGL9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b591250>"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Discussion\n",
      "\n",
      "*Note: these discussion questions are mostly to get you thinking, it will be read but not graded*\n",
      "\n",
      "Discuss the following questions and any other results you found interesting.\n",
      "\n",
      "Does your model achieve reasonable performance over the prior?\n",
      "\n",
      "Does the order of feature engineering matter?\n",
      "\n",
      "Does it look like adding more features would improve your model results?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**This model does achieve reasonable performance over the prior.  If we were to guess that y=1, we'd have an accuracy of 67%. Using this model, we achieve an accuracy of 86%.  This seems to max out after finding the top 8 features using forward selection.**\n",
      "\n",
      "**After trying different orders of feature engineering, I did find that performing the operations in different orders does change the outcome/success of the model.**\n",
      "\n",
      "**From the graph above, It seems like addint more features to this model will not change the results.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}