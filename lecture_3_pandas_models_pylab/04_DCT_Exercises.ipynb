{
 "metadata": {
  "name": "",
  "signature": "sha256:5dc67c74a4a79d4442c883480f505f37b1cf503b9c5561dc0f6a32344e84012d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#DCT Analysis\n",
      "\n",
      "Last week we dealt with the KDD 1998 dataset which concerend donations to a charity.  The data set has a lot of non numeric features which require complicated coding as you saw.  This week we use an anonymized purely numeric dataset so we can skip the complications of real data and move onto modeling.\n",
      "\n",
      "The data is sourced from http://mlcomp.org/datasets/1571 and concerns identifying the gender (an anonymized binary flag -1/1) of people based on 800 anonymized numeric features.  Accoridng the the MLComp website, error rates of ~10% were found using linear models.  We hope to do better using some simple feature selection and feature engineering.\n",
      "\n",
      "We have provided a tab delimited version of the test/train data set in the data directory for your use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import the libraries \n",
      "%pylab inline\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the train and test data into two data frames (`train_data` and `test_data`). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data = pd.read_csv(\"data/train\",sep=\"\\t\")\n",
      "test_data  = pd.read_csv(\"data/test\",sep=\"\\t\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Separate the `label` column from `train_data` into two new data frames called `train_x` and `train_y`. `train_y` is to contain only the column titled `label` in `train_data` while `train_x` should contain all of the rest. `train_x` should contains 800 columns, check that this is the case.\n",
      "\n",
      "In a similar way partition `test_data` into two new data frames called `test_x` and `test_y`\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_y = pd.DataFrame(train_data['label'])\n",
      "train_x = train_data.drop('label',axis=1)\n",
      "\n",
      "test_y = pd.DataFrame(test_data['label'])\n",
      "test_x = test_data.drop('label',axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Summarizing the Data\n",
      "Use pandas to describe the `train_x` data frame.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_x.describe()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>F_790</th>\n",
        "      <th>F_791</th>\n",
        "      <th>F_792</th>\n",
        "      <th>F_793</th>\n",
        "      <th>F_794</th>\n",
        "      <th>F_795</th>\n",
        "      <th>F_796</th>\n",
        "      <th>F_797</th>\n",
        "      <th>F_798</th>\n",
        "      <th>F_799</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> -85.409660</td>\n",
        "      <td>   5.325581</td>\n",
        "      <td>   1.905188</td>\n",
        "      <td>   0.502683</td>\n",
        "      <td>   1.611807</td>\n",
        "      <td>   0.316637</td>\n",
        "      <td>  -0.064401</td>\n",
        "      <td>  -0.050089</td>\n",
        "      <td>   0.429338</td>\n",
        "      <td>  -0.032200</td>\n",
        "      <td>...</td>\n",
        "      <td>   6.978533</td>\n",
        "      <td>   2.423971</td>\n",
        "      <td>   4.769231</td>\n",
        "      <td>   1.019678</td>\n",
        "      <td>  -0.674419</td>\n",
        "      <td>  -4.341682</td>\n",
        "      <td> -20.626118</td>\n",
        "      <td>  -2.878354</td>\n",
        "      <td>   4.332737</td>\n",
        "      <td>  -0.135957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>  58.830922</td>\n",
        "      <td>  16.511576</td>\n",
        "      <td>   7.158970</td>\n",
        "      <td>   2.981949</td>\n",
        "      <td>   6.181943</td>\n",
        "      <td>   4.716507</td>\n",
        "      <td>   4.097311</td>\n",
        "      <td>   2.891279</td>\n",
        "      <td>   2.600992</td>\n",
        "      <td>   2.107939</td>\n",
        "      <td>...</td>\n",
        "      <td>  97.125051</td>\n",
        "      <td>  89.087333</td>\n",
        "      <td> 103.323683</td>\n",
        "      <td> 129.015133</td>\n",
        "      <td> 113.575980</td>\n",
        "      <td>  92.834494</td>\n",
        "      <td>  78.890002</td>\n",
        "      <td>  72.975075</td>\n",
        "      <td>  82.473457</td>\n",
        "      <td>  89.108263</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>-157.000000</td>\n",
        "      <td> -40.000000</td>\n",
        "      <td> -31.000000</td>\n",
        "      <td> -23.000000</td>\n",
        "      <td> -14.000000</td>\n",
        "      <td> -28.000000</td>\n",
        "      <td> -25.000000</td>\n",
        "      <td> -15.000000</td>\n",
        "      <td>  -9.000000</td>\n",
        "      <td> -10.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>-287.000000</td>\n",
        "      <td>-279.000000</td>\n",
        "      <td>-368.000000</td>\n",
        "      <td>-375.000000</td>\n",
        "      <td>-313.000000</td>\n",
        "      <td>-323.000000</td>\n",
        "      <td>-284.000000</td>\n",
        "      <td>-277.000000</td>\n",
        "      <td>-314.000000</td>\n",
        "      <td>-237.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>-120.500000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> -60.500000</td>\n",
        "      <td> -54.500000</td>\n",
        "      <td> -62.500000</td>\n",
        "      <td> -88.000000</td>\n",
        "      <td> -72.000000</td>\n",
        "      <td> -62.500000</td>\n",
        "      <td> -71.000000</td>\n",
        "      <td> -47.500000</td>\n",
        "      <td> -49.500000</td>\n",
        "      <td> -58.500000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>-101.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>  10.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   7.000000</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>  -5.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td> -16.000000</td>\n",
        "      <td>  -5.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>  -2.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> -73.500000</td>\n",
        "      <td>   5.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>  77.000000</td>\n",
        "      <td>  56.000000</td>\n",
        "      <td>  75.000000</td>\n",
        "      <td>  90.500000</td>\n",
        "      <td>  75.000000</td>\n",
        "      <td>  59.000000</td>\n",
        "      <td>  30.500000</td>\n",
        "      <td>  43.000000</td>\n",
        "      <td>  61.000000</td>\n",
        "      <td>  57.500000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 158.000000</td>\n",
        "      <td>  80.000000</td>\n",
        "      <td>  42.000000</td>\n",
        "      <td>  17.000000</td>\n",
        "      <td>  49.000000</td>\n",
        "      <td>  25.000000</td>\n",
        "      <td>  18.000000</td>\n",
        "      <td>  15.000000</td>\n",
        "      <td>  29.000000</td>\n",
        "      <td>  12.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 299.000000</td>\n",
        "      <td> 272.000000</td>\n",
        "      <td> 297.000000</td>\n",
        "      <td> 373.000000</td>\n",
        "      <td> 458.000000</td>\n",
        "      <td> 284.000000</td>\n",
        "      <td> 257.000000</td>\n",
        "      <td> 247.000000</td>\n",
        "      <td> 293.000000</td>\n",
        "      <td> 274.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 800 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "              F_0         F_1         F_2         F_3         F_4         F_5  \\\n",
        "count  559.000000  559.000000  559.000000  559.000000  559.000000  559.000000   \n",
        "mean   -85.409660    5.325581    1.905188    0.502683    1.611807    0.316637   \n",
        "std     58.830922   16.511576    7.158970    2.981949    6.181943    4.716507   \n",
        "min   -157.000000  -40.000000  -31.000000  -23.000000  -14.000000  -28.000000   \n",
        "25%   -120.500000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
        "50%   -101.000000    1.000000    1.000000    0.000000    0.000000    0.000000   \n",
        "75%    -73.500000    5.000000    2.000000    1.000000    3.000000    2.000000   \n",
        "max    158.000000   80.000000   42.000000   17.000000   49.000000   25.000000   \n",
        "\n",
        "              F_6         F_7         F_8         F_9     ...           F_790  \\\n",
        "count  559.000000  559.000000  559.000000  559.000000     ...      559.000000   \n",
        "mean    -0.064401   -0.050089    0.429338   -0.032200     ...        6.978533   \n",
        "std      4.097311    2.891279    2.600992    2.107939     ...       97.125051   \n",
        "min    -25.000000  -15.000000   -9.000000  -10.000000     ...     -287.000000   \n",
        "25%     -1.000000   -1.000000   -1.000000   -1.000000     ...      -60.500000   \n",
        "50%      0.000000    0.000000    0.000000    0.000000     ...       10.000000   \n",
        "75%      1.000000    1.000000    1.000000    1.000000     ...       77.000000   \n",
        "max     18.000000   15.000000   29.000000   12.000000     ...      299.000000   \n",
        "\n",
        "            F_791       F_792       F_793       F_794       F_795       F_796  \\\n",
        "count  559.000000  559.000000  559.000000  559.000000  559.000000  559.000000   \n",
        "mean     2.423971    4.769231    1.019678   -0.674419   -4.341682  -20.626118   \n",
        "std     89.087333  103.323683  129.015133  113.575980   92.834494   78.890002   \n",
        "min   -279.000000 -368.000000 -375.000000 -313.000000 -323.000000 -284.000000   \n",
        "25%    -54.500000  -62.500000  -88.000000  -72.000000  -62.500000  -71.000000   \n",
        "50%      0.000000    7.000000    3.000000   -5.000000    1.000000  -16.000000   \n",
        "75%     56.000000   75.000000   90.500000   75.000000   59.000000   30.500000   \n",
        "max    272.000000  297.000000  373.000000  458.000000  284.000000  257.000000   \n",
        "\n",
        "            F_797       F_798       F_799  \n",
        "count  559.000000  559.000000  559.000000  \n",
        "mean    -2.878354    4.332737   -0.135957  \n",
        "std     72.975075   82.473457   89.108263  \n",
        "min   -277.000000 -314.000000 -237.000000  \n",
        "25%    -47.500000  -49.500000  -58.500000  \n",
        "50%     -5.000000    1.000000   -2.000000  \n",
        "75%     43.000000   61.000000   57.500000  \n",
        "max    247.000000  293.000000  274.000000  \n",
        "\n",
        "[8 rows x 800 columns]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the range of each column varies by a significant amount.  Use the Z-Scaling method to normalize the data and make it suitable to modeling. More specifically, for each column in `train_x` compute the average $\\mu$ and the standard deviation $\\sigma$. Then transform each value in the column from the original value $X$ to the Z-value $Z$ using the formula:\n",
      "\n",
      "$$ Z=\\frac{X-\\mu}{\\sigma} $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get list of column names\n",
      "colIndx = train_x.columns\n",
      "\n",
      "# scale both the train AND test data:\n",
      "# loop through column names, doing vectorised z-scaling\n",
      "# note:\n",
      "#   I compute the standard dev. and average using the 'train' data\n",
      "#   and use these vals to scale the 'test' data\n",
      "for thisCol in colIndx:\n",
      "    thisAvg = train_x[thisCol].mean()   # train mean\n",
      "    thisStd = train_x[thisCol].std()    # treain std dev\n",
      "    # scale train data\n",
      "    train_x[thisCol] = (train_x[thisCol]-thisAvg) / thisStd\n",
      "    # scale test data\n",
      "    test_x[thisCol] = (test_x[thisCol]-thisAvg) / thisStd\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the values of $\\mu$ and $\\sigma$ computed from `train_x` to scale the values in `test_x`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I scaled 'test_x' above, when scaling 'train_x'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Afterwards summarize the train_x and test_x post processing.  Train_x should have exactly 0 mean and 1 standard deviation while test_x should be nearly there."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>F_790</th>\n",
        "      <th>F_791</th>\n",
        "      <th>F_792</th>\n",
        "      <th>F_793</th>\n",
        "      <th>F_794</th>\n",
        "      <th>F_795</th>\n",
        "      <th>F_796</th>\n",
        "      <th>F_797</th>\n",
        "      <th>F_798</th>\n",
        "      <th>F_799</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td>...</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> 8.262125e-17</td>\n",
        "      <td> 3.495514e-17</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>-1.271096e-17</td>\n",
        "      <td> 1.588870e-18</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td> 2.383305e-17</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td> 5.561046e-18</td>\n",
        "      <td> 6.355481e-18</td>\n",
        "      <td>...</td>\n",
        "      <td>-3.177740e-17</td>\n",
        "      <td> 2.065531e-17</td>\n",
        "      <td>-2.542192e-17</td>\n",
        "      <td>-2.542192e-17</td>\n",
        "      <td>-1.271096e-17</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td> 3.813288e-17</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td>-6.355481e-18</td>\n",
        "      <td> 7.944351e-19</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>...</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>-1.216883e+00</td>\n",
        "      <td>-2.745079e+00</td>\n",
        "      <td>  -4.596358</td>\n",
        "      <td>-7.881653e+00</td>\n",
        "      <td>-2.525388e+00</td>\n",
        "      <td>-6.003730e+00</td>\n",
        "      <td>-6.085844e+00</td>\n",
        "      <td>-5.170691e+00</td>\n",
        "      <td>-3.625285e+00</td>\n",
        "      <td>-4.728695e+00</td>\n",
        "      <td>...</td>\n",
        "      <td>-3.026804e+00</td>\n",
        "      <td>-3.158967e+00</td>\n",
        "      <td>-3.607781e+00</td>\n",
        "      <td>-2.914539e+00</td>\n",
        "      <td>-2.749926e+00</td>\n",
        "      <td>-3.432542e+00</td>\n",
        "      <td>-3.338495e+00</td>\n",
        "      <td>-3.756374e+00</td>\n",
        "      <td>-3.859820e+00</td>\n",
        "      <td>-2.658160e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>-5.964608e-01</td>\n",
        "      <td>-3.830998e-01</td>\n",
        "      <td>  -0.405811</td>\n",
        "      <td>-5.039266e-01</td>\n",
        "      <td>-4.224896e-01</td>\n",
        "      <td>-2.791551e-01</td>\n",
        "      <td>-2.283447e-01</td>\n",
        "      <td>-3.285434e-01</td>\n",
        "      <td>-5.495358e-01</td>\n",
        "      <td>-4.591213e-01</td>\n",
        "      <td>...</td>\n",
        "      <td>-6.947593e-01</td>\n",
        "      <td>-6.389682e-01</td>\n",
        "      <td>-6.510534e-01</td>\n",
        "      <td>-6.899941e-01</td>\n",
        "      <td>-6.279988e-01</td>\n",
        "      <td>-6.264732e-01</td>\n",
        "      <td>-6.385332e-01</td>\n",
        "      <td>-6.114642e-01</td>\n",
        "      <td>-6.527280e-01</td>\n",
        "      <td>-6.549790e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>-2.650025e-01</td>\n",
        "      <td>-2.619726e-01</td>\n",
        "      <td>  -0.126441</td>\n",
        "      <td>-1.685755e-01</td>\n",
        "      <td>-2.607282e-01</td>\n",
        "      <td>-6.713376e-02</td>\n",
        "      <td> 1.571780e-02</td>\n",
        "      <td> 1.732432e-02</td>\n",
        "      <td>-1.650671e-01</td>\n",
        "      <td> 1.527576e-02</td>\n",
        "      <td>...</td>\n",
        "      <td> 3.110904e-02</td>\n",
        "      <td>-2.720893e-02</td>\n",
        "      <td> 2.159011e-02</td>\n",
        "      <td> 1.534953e-02</td>\n",
        "      <td>-3.808535e-02</td>\n",
        "      <td> 5.753984e-02</td>\n",
        "      <td> 5.864011e-02</td>\n",
        "      <td>-2.907357e-02</td>\n",
        "      <td>-4.040981e-02</td>\n",
        "      <td>-2.091886e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 2.024388e-01</td>\n",
        "      <td>-1.971837e-02</td>\n",
        "      <td>   0.013244</td>\n",
        "      <td> 1.667757e-01</td>\n",
        "      <td> 2.245561e-01</td>\n",
        "      <td> 3.569089e-01</td>\n",
        "      <td> 2.597803e-01</td>\n",
        "      <td> 3.631920e-01</td>\n",
        "      <td> 2.194016e-01</td>\n",
        "      <td> 4.896728e-01</td>\n",
        "      <td>...</td>\n",
        "      <td> 7.209414e-01</td>\n",
        "      <td> 6.013877e-01</td>\n",
        "      <td> 6.797161e-01</td>\n",
        "      <td> 6.935645e-01</td>\n",
        "      <td> 6.662889e-01</td>\n",
        "      <td> 6.823076e-01</td>\n",
        "      <td> 6.480684e-01</td>\n",
        "      <td> 6.286853e-01</td>\n",
        "      <td> 6.870970e-01</td>\n",
        "      <td> 6.468082e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 4.137444e+00</td>\n",
        "      <td> 4.522549e+00</td>\n",
        "      <td>   5.600640</td>\n",
        "      <td> 5.532395e+00</td>\n",
        "      <td> 7.665582e+00</td>\n",
        "      <td> 5.233399e+00</td>\n",
        "      <td> 4.408843e+00</td>\n",
        "      <td> 5.205340e+00</td>\n",
        "      <td> 1.098453e+01</td>\n",
        "      <td> 5.708041e+00</td>\n",
        "      <td>...</td>\n",
        "      <td> 3.006654e+00</td>\n",
        "      <td> 3.025975e+00</td>\n",
        "      <td> 2.828304e+00</td>\n",
        "      <td> 2.883230e+00</td>\n",
        "      <td> 4.038481e+00</td>\n",
        "      <td> 3.105976e+00</td>\n",
        "      <td> 3.519155e+00</td>\n",
        "      <td> 3.424160e+00</td>\n",
        "      <td> 3.500123e+00</td>\n",
        "      <td> 3.076437e+00</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 800 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "                F_0           F_1         F_2           F_3           F_4  \\\n",
        "count  5.590000e+02  5.590000e+02  559.000000  5.590000e+02  5.590000e+02   \n",
        "mean   8.262125e-17  3.495514e-17    0.000000 -1.271096e-17  1.588870e-18   \n",
        "std    1.000000e+00  1.000000e+00    1.000000  1.000000e+00  1.000000e+00   \n",
        "min   -1.216883e+00 -2.745079e+00   -4.596358 -7.881653e+00 -2.525388e+00   \n",
        "25%   -5.964608e-01 -3.830998e-01   -0.405811 -5.039266e-01 -4.224896e-01   \n",
        "50%   -2.650025e-01 -2.619726e-01   -0.126441 -1.685755e-01 -2.607282e-01   \n",
        "75%    2.024388e-01 -1.971837e-02    0.013244  1.667757e-01  2.245561e-01   \n",
        "max    4.137444e+00  4.522549e+00    5.600640  5.532395e+00  7.665582e+00   \n",
        "\n",
        "                F_5           F_6           F_7           F_8           F_9  \\\n",
        "count  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean   1.271096e-17  2.383305e-17  1.271096e-17  5.561046e-18  6.355481e-18   \n",
        "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min   -6.003730e+00 -6.085844e+00 -5.170691e+00 -3.625285e+00 -4.728695e+00   \n",
        "25%   -2.791551e-01 -2.283447e-01 -3.285434e-01 -5.495358e-01 -4.591213e-01   \n",
        "50%   -6.713376e-02  1.571780e-02  1.732432e-02 -1.650671e-01  1.527576e-02   \n",
        "75%    3.569089e-01  2.597803e-01  3.631920e-01  2.194016e-01  4.896728e-01   \n",
        "max    5.233399e+00  4.408843e+00  5.205340e+00  1.098453e+01  5.708041e+00   \n",
        "\n",
        "           ...              F_790         F_791         F_792         F_793  \\\n",
        "count      ...       5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean       ...      -3.177740e-17  2.065531e-17 -2.542192e-17 -2.542192e-17   \n",
        "std        ...       1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min        ...      -3.026804e+00 -3.158967e+00 -3.607781e+00 -2.914539e+00   \n",
        "25%        ...      -6.947593e-01 -6.389682e-01 -6.510534e-01 -6.899941e-01   \n",
        "50%        ...       3.110904e-02 -2.720893e-02  2.159011e-02  1.534953e-02   \n",
        "75%        ...       7.209414e-01  6.013877e-01  6.797161e-01  6.935645e-01   \n",
        "max        ...       3.006654e+00  3.025975e+00  2.828304e+00  2.883230e+00   \n",
        "\n",
        "              F_794         F_795         F_796         F_797         F_798  \\\n",
        "count  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean  -1.271096e-17  1.271096e-17  3.813288e-17  1.271096e-17 -6.355481e-18   \n",
        "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min   -2.749926e+00 -3.432542e+00 -3.338495e+00 -3.756374e+00 -3.859820e+00   \n",
        "25%   -6.279988e-01 -6.264732e-01 -6.385332e-01 -6.114642e-01 -6.527280e-01   \n",
        "50%   -3.808535e-02  5.753984e-02  5.864011e-02 -2.907357e-02 -4.040981e-02   \n",
        "75%    6.662889e-01  6.823076e-01  6.480684e-01  6.286853e-01  6.870970e-01   \n",
        "max    4.038481e+00  3.105976e+00  3.519155e+00  3.424160e+00  3.500123e+00   \n",
        "\n",
        "              F_799  \n",
        "count  5.590000e+02  \n",
        "mean   7.944351e-19  \n",
        "std    1.000000e+00  \n",
        "min   -2.658160e+00  \n",
        "25%   -6.549790e-01  \n",
        "50%   -2.091886e-02  \n",
        "75%    6.468082e-01  \n",
        "max    3.076437e+00  \n",
        "\n",
        "[8 rows x 800 columns]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>F_790</th>\n",
        "      <th>F_791</th>\n",
        "      <th>F_792</th>\n",
        "      <th>F_793</th>\n",
        "      <th>F_794</th>\n",
        "      <th>F_795</th>\n",
        "      <th>F_796</th>\n",
        "      <th>F_797</th>\n",
        "      <th>F_798</th>\n",
        "      <th>F_799</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>  -0.048084</td>\n",
        "      <td>   0.077335</td>\n",
        "      <td>   0.020257</td>\n",
        "      <td>   0.072765</td>\n",
        "      <td>   0.021508</td>\n",
        "      <td>  -0.053827</td>\n",
        "      <td>   0.139281</td>\n",
        "      <td>  -0.043456</td>\n",
        "      <td>  -0.033157</td>\n",
        "      <td>   0.011306</td>\n",
        "      <td>...</td>\n",
        "      <td>  -0.018519</td>\n",
        "      <td>   0.013229</td>\n",
        "      <td>   0.078567</td>\n",
        "      <td>   0.026311</td>\n",
        "      <td>  -0.057758</td>\n",
        "      <td>  -0.149649</td>\n",
        "      <td>  -0.011793</td>\n",
        "      <td>   0.047642</td>\n",
        "      <td>   0.103214</td>\n",
        "      <td>   0.064352</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>   0.922365</td>\n",
        "      <td>   0.932226</td>\n",
        "      <td>   0.982286</td>\n",
        "      <td>   1.007044</td>\n",
        "      <td>   1.061437</td>\n",
        "      <td>   1.143097</td>\n",
        "      <td>   1.196676</td>\n",
        "      <td>   1.101199</td>\n",
        "      <td>   0.952202</td>\n",
        "      <td>   1.244538</td>\n",
        "      <td>...</td>\n",
        "      <td>   1.082358</td>\n",
        "      <td>   1.052719</td>\n",
        "      <td>   1.043850</td>\n",
        "      <td>   1.052912</td>\n",
        "      <td>   1.096577</td>\n",
        "      <td>   0.964285</td>\n",
        "      <td>   0.996227</td>\n",
        "      <td>   1.051661</td>\n",
        "      <td>   0.913283</td>\n",
        "      <td>   0.939877</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>  -1.080900</td>\n",
        "      <td>  -2.866206</td>\n",
        "      <td>  -3.199509</td>\n",
        "      <td>  -4.863492</td>\n",
        "      <td>  -4.143003</td>\n",
        "      <td>  -7.063837</td>\n",
        "      <td>  -5.109595</td>\n",
        "      <td>  -6.554162</td>\n",
        "      <td>  -4.394223</td>\n",
        "      <td>  -5.677489</td>\n",
        "      <td>...</td>\n",
        "      <td>  -3.253317</td>\n",
        "      <td>  -3.069168</td>\n",
        "      <td>  -2.668984</td>\n",
        "      <td>  -2.682008</td>\n",
        "      <td>  -3.128528</td>\n",
        "      <td>  -3.367911</td>\n",
        "      <td>  -3.477930</td>\n",
        "      <td>  -2.824549</td>\n",
        "      <td>  -2.829186</td>\n",
        "      <td>  -2.130712</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>  -0.587962</td>\n",
        "      <td>  -0.383100</td>\n",
        "      <td>  -0.266126</td>\n",
        "      <td>  -0.168575</td>\n",
        "      <td>  -0.422490</td>\n",
        "      <td>  -0.491176</td>\n",
        "      <td>  -0.228345</td>\n",
        "      <td>  -0.328543</td>\n",
        "      <td>  -0.549536</td>\n",
        "      <td>  -0.459121</td>\n",
        "      <td>...</td>\n",
        "      <td>  -0.771979</td>\n",
        "      <td>  -0.678255</td>\n",
        "      <td>  -0.525235</td>\n",
        "      <td>  -0.744251</td>\n",
        "      <td>  -0.694034</td>\n",
        "      <td>  -0.755735</td>\n",
        "      <td>  -0.537126</td>\n",
        "      <td>  -0.611464</td>\n",
        "      <td>  -0.507227</td>\n",
        "      <td>  -0.553978</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>  -0.366990</td>\n",
        "      <td>  -0.201409</td>\n",
        "      <td>  -0.126441</td>\n",
        "      <td>  -0.168575</td>\n",
        "      <td>  -0.098967</td>\n",
        "      <td>  -0.067134</td>\n",
        "      <td>   0.015718</td>\n",
        "      <td>   0.017324</td>\n",
        "      <td>  -0.165067</td>\n",
        "      <td>   0.015276</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.031109</td>\n",
        "      <td>   0.017691</td>\n",
        "      <td>   0.021590</td>\n",
        "      <td>   0.069607</td>\n",
        "      <td>  -0.038085</td>\n",
        "      <td>  -0.114810</td>\n",
        "      <td>  -0.004739</td>\n",
        "      <td>   0.012036</td>\n",
        "      <td>   0.117217</td>\n",
        "      <td>   0.023970</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>   0.083454</td>\n",
        "      <td>   0.040845</td>\n",
        "      <td>   0.013244</td>\n",
        "      <td>   0.166776</td>\n",
        "      <td>   0.224556</td>\n",
        "      <td>   0.356909</td>\n",
        "      <td>   0.259780</td>\n",
        "      <td>   0.363192</td>\n",
        "      <td>   0.219402</td>\n",
        "      <td>   0.489673</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.720941</td>\n",
        "      <td>   0.629450</td>\n",
        "      <td>   0.699073</td>\n",
        "      <td>   0.794328</td>\n",
        "      <td>   0.697105</td>\n",
        "      <td>   0.380696</td>\n",
        "      <td>   0.591027</td>\n",
        "      <td>   0.731460</td>\n",
        "      <td>   0.662847</td>\n",
        "      <td>   0.674864</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>   3.763491</td>\n",
        "      <td>   3.856350</td>\n",
        "      <td>   4.064106</td>\n",
        "      <td>   3.855639</td>\n",
        "      <td>   5.562684</td>\n",
        "      <td>   4.597335</td>\n",
        "      <td>   7.093530</td>\n",
        "      <td>   6.242943</td>\n",
        "      <td>   3.679620</td>\n",
        "      <td>   8.080026</td>\n",
        "      <td>...</td>\n",
        "      <td>   3.593527</td>\n",
        "      <td>   2.790251</td>\n",
        "      <td>   3.679996</td>\n",
        "      <td>   2.890981</td>\n",
        "      <td>   2.779412</td>\n",
        "      <td>   2.858223</td>\n",
        "      <td>   2.885361</td>\n",
        "      <td>   3.218611</td>\n",
        "      <td>   2.736241</td>\n",
        "      <td>   2.694879</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 800 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "              F_0         F_1         F_2         F_3         F_4         F_5  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000  239.000000  239.000000   \n",
        "mean    -0.048084    0.077335    0.020257    0.072765    0.021508   -0.053827   \n",
        "std      0.922365    0.932226    0.982286    1.007044    1.061437    1.143097   \n",
        "min     -1.080900   -2.866206   -3.199509   -4.863492   -4.143003   -7.063837   \n",
        "25%     -0.587962   -0.383100   -0.266126   -0.168575   -0.422490   -0.491176   \n",
        "50%     -0.366990   -0.201409   -0.126441   -0.168575   -0.098967   -0.067134   \n",
        "75%      0.083454    0.040845    0.013244    0.166776    0.224556    0.356909   \n",
        "max      3.763491    3.856350    4.064106    3.855639    5.562684    4.597335   \n",
        "\n",
        "              F_6         F_7         F_8         F_9     ...           F_790  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000     ...      239.000000   \n",
        "mean     0.139281   -0.043456   -0.033157    0.011306     ...       -0.018519   \n",
        "std      1.196676    1.101199    0.952202    1.244538     ...        1.082358   \n",
        "min     -5.109595   -6.554162   -4.394223   -5.677489     ...       -3.253317   \n",
        "25%     -0.228345   -0.328543   -0.549536   -0.459121     ...       -0.771979   \n",
        "50%      0.015718    0.017324   -0.165067    0.015276     ...        0.031109   \n",
        "75%      0.259780    0.363192    0.219402    0.489673     ...        0.720941   \n",
        "max      7.093530    6.242943    3.679620    8.080026     ...        3.593527   \n",
        "\n",
        "            F_791       F_792       F_793       F_794       F_795       F_796  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000  239.000000  239.000000   \n",
        "mean     0.013229    0.078567    0.026311   -0.057758   -0.149649   -0.011793   \n",
        "std      1.052719    1.043850    1.052912    1.096577    0.964285    0.996227   \n",
        "min     -3.069168   -2.668984   -2.682008   -3.128528   -3.367911   -3.477930   \n",
        "25%     -0.678255   -0.525235   -0.744251   -0.694034   -0.755735   -0.537126   \n",
        "50%      0.017691    0.021590    0.069607   -0.038085   -0.114810   -0.004739   \n",
        "75%      0.629450    0.699073    0.794328    0.697105    0.380696    0.591027   \n",
        "max      2.790251    3.679996    2.890981    2.779412    2.858223    2.885361   \n",
        "\n",
        "            F_797       F_798       F_799  \n",
        "count  239.000000  239.000000  239.000000  \n",
        "mean     0.047642    0.103214    0.064352  \n",
        "std      1.051661    0.913283    0.939877  \n",
        "min     -2.824549   -2.829186   -2.130712  \n",
        "25%     -0.611464   -0.507227   -0.553978  \n",
        "50%      0.012036    0.117217    0.023970  \n",
        "75%      0.731460    0.662847    0.674864  \n",
        "max      3.218611    2.736241    2.694879  \n",
        "\n",
        "[8 rows x 800 columns]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Studying The Target\n",
      "\n",
      "Now that we have normalized the input matrix, we move on to looking at the target.  Within the train_y data frame, what is the percentage of positive examples?  This is our prior probability and the accuracy we will attain if we always guess positive.  Also measure the prior for the test_y frame and comment if there are major differences between the testing/training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Since (1==True) and this is a binary +/- 1 flag, \n",
      "# this gives the percent of positive\n",
      "100* float(sum(train_y['label'] >0 ))/ len(train_y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "49.37388193202147"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The same as above for test_y\n",
      "100* float(sum(test_y['label'] >0 ))/ len(test_y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "51.46443514644351"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__These two percentages are similar enough targets that I'm willing to proceed with the modeling without any further investigation.__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Supervised Modeling\n",
      "\n",
      "We choose to use logistic regression on this data set since it is a simple binary classification problem.  In this section we aim to train a logistic regression model that accurately predicts the class of each record.  We'll train using the train_x/train_y data frames and evaluate on the test_x/test_y data frames.\n",
      "\n",
      "###Baseline Model\n",
      "\n",
      "Use sklearn's linear model package to train a logistic regression model using the whole train_x/train_y data frames.  Then evaluate this model using the test_x/test_y data frames by computing the accuracy (% of correct predictions).  Store this computed accuracy as acc_baseline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.linear_model  \n",
      "#train model and evaluate performance here\n",
      "\n",
      "# training the model\n",
      "model = sklearn.linear_model.LogisticRegression()\n",
      "model.fit(train_x, train_y['label'])              # training the model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# checking the model's accuracy on the test data\n",
      "acc_baseline = model.score(test_x, test_y['label'])\n",
      "\n",
      "acc_baseline\n",
      "# 92%\n",
      "# I expected this number to be good considering we're using all 800 parameters in the model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.91631799163179917"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__I'm doing the following check to make sure I understand what 'model.score' returns.__"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# using this model to predict, and comparing the two columns in a new dataframe\n",
      "#  and making sure this returns the same percent as model.score above\n",
      "predicted_y = model.predict(test_x) # output is predicted test_y\n",
      "compDF = pd.DataFrame()\n",
      "compDF['predict'] = predicted_y\n",
      "compDF['actual']  = test_y.label\n",
      "compDF['correct'] = compDF.predict == compDF.actual\n",
      "\n",
      "float(sum(compDF['correct']))/len(compDF)  # this should give the same as model.score\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "0.9163179916317992"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Simpler model \n",
      "\n",
      "While your previous model has good performance, it uses 800 features.  The high feature count means the model is difficult to explain and may result in overfitting (since there are under 800 records).  We wish to build a model with 20 features which also has good accuracy.\n",
      "\n",
      "As a first step towards this, find the 20 features which have the largest absolute coefficients in your old model and build a logistic regression model which only uses these 20 features.  Then measure the accuracy of this new model and comment on the impact to performance caused by just using these 20 features.\n",
      "\n",
      "To do this you will have to make copies of the train_x/test_x data frames which only have the 20 highest coefficient features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get list of top 20 predictive features\n",
      "coeffs = model.coef_[0]                     # coeffs for the model\n",
      "allIndex = np.argsort(abs(coeffs))          # index of abs(all_coeffs) in ascending order\n",
      "keepIndex = allIndex[::-1][:20]             # reverse order and take top 20\n",
      "twenty_best = train_x.columns[keepIndex]    # column names of top 20\n",
      "\n",
      "# reducing {train,test}_x to 20 best columns\n",
      "train_x_twenty_best = train_x[twenty_best]\n",
      "test_x_twenty_best = test_x[twenty_best]\n",
      "\n",
      "#Train model/evaluate performance here\n",
      "model_20best = model.fit(train_x_twenty_best, train_y['label'])\n",
      "\n",
      "top20_score = model_20best.score(test_x_twenty_best, test_y)\n",
      "\n",
      "top20_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "0.89121338912133896"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "__We now have a model that uses only 20 features and is has 89% accuracy, which is a better than overfitting, using 800 featers with a 92% accuracy.__\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Basic Feature Engineering\n",
      "On the last homework we discussed some text -> numeric feature engineering functions which are commonly used; however, feature engineering is a wide field which is often much simpler.  In particular when you already have all numeric features simple mathematical functions often add significant power to your features.  This helps account for non linear relationships between the features and the target.  \n",
      "\n",
      "Commonly used functions include:\n",
      "\n",
      "1. Log\n",
      "2. Square root\n",
      "3. Squareing\n",
      "4. Sign (1 if the number is positive, 0 otherwise)\n",
      "5. Absolute Value\n",
      "6. Binning\n",
      "\n",
      "Make for each feature (FEATURE) in train_x/test_x create 3 new features: the square of the feature (FEATURE_SQ), the absolute value of the feature (FEATURE_ABS), and the sign of the feature (FEAUTRE_SIGN).  Thus you should have 3200 features in your test_x/train_x data frames."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for col in train_x.columns:\n",
      "    train_x[col + \"_SQ\"]   = train_x[col]**2\n",
      "    train_x[col + \"_ABS\"]  = abs(train_x[col])\n",
      "    train_x[col + \"_SIGN\"] = np.sign(train_x[col])\n",
      "    \n",
      "for col in test_x.columns:\n",
      "    test_x[col + \"_SQ\"]   = test_x[col]**2\n",
      "    test_x[col + \"_ABS\"]  = abs(test_x[col])\n",
      "    test_x[col + \"_SIGN\"] = np.sign(test_x[col])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Shape of train_x:\", shape(train_x)\n",
      "print \"Shape of test_x :\", shape(test_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shape of train_x: (559, 3200)\n",
        "Shape of test_x : (239, 3200)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###PCA Feature Engineering\n",
      "\n",
      "These simple techniques allow non linear relationships between the features and the target to be captured.  However, one issue is that these features are all univariate and thus our model is unable to capture relationships of multiple variables to the target.  A common technique is to use product/ratio feature (IE F_0*F_1 or F_0/F_1); however, this becomes intractable since this will produced $O(n^2)$ features for $n$ input features.  Domain expertise can give you ideas on which interactions are likely important, but cannot help you find new insights.  \n",
      "\n",
      "One idea is to use unsupervised learning techniques such as PCA to try and capture these interactions.  Find the top 10 prinipal components of train_x and then add the PCA coefficints for train_x/test_x as features PCA_0, PCA_1, ..., PCA_9.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# find top 10 principal components of train_x\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=10, whiten=True)\n",
      "pca.fit(train_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "PCA(copy=True, n_components=10, whiten=True)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get pca coefficients for train_x and test_x\n",
      "train_x_coeffs = pca.transform(train_x).T\n",
      "test_x_coeffs  = pca.transform(test_x).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add pca coefficients for train_x, test_x as features\n",
      "\n",
      "# make the pca column index strings\n",
      "pcaIndxArr = ['PCA_'+str(ii) for ii in range(10)]\n",
      "\n",
      "# adding pca coefficients to train_x\n",
      "for colName, coeffList in zip(pcaIndxArr, train_x_coeffs):\n",
      "    train_x[colName] = coeffList\n",
      "\n",
      "# adding pca coefficients to test_x\n",
      "for colName, coeffList in zip(pcaIndxArr, test_x_coeffs):\n",
      "    test_x[colName] = coeffList\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Feature Selection\n",
      "\n",
      "Earlier, we built a model with the top 20 features from our whole model.  This is a possible way of doing [feature selection](http://en.wikipedia.org/wiki/Feature_selection), or choosing a subset of features to build our model with.  However, this is not the best way for a variety of reasons which you'll come to understand in future classes.  \n",
      "\n",
      "One better method of feature which is easy to understand is [forward selection](http://en.wikipedia.org/wiki/Stepwise_regression).  In this process you start with a model with no features and then search for the single best feature to add.  After you've found it you keep searching for the next feature to add until some stopping criteria is reached.  In this part we will guide you through the process of writing such an algorithm.\n",
      "\n",
      "As a first step, write code which will find the best 1 feature model.  In otherwords, for each feature train a model and evaluate its acuracy on the test data.  Store the best model, the feature used, and its accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use a logistic regression loop on the train_x data\n",
      "# compute the score for each model on test_y\n",
      "# hold onto the column name and accuracy for best feature\n",
      "\n",
      "import sklearn.linear_model  \n",
      "thisModel = sklearn.linear_model.LogisticRegression() # instantiate the model\n",
      "\n",
      "bestOneScore = -1.0\n",
      "bestOneCol   = 'Na'\n",
      "for thisCol in train_x.columns:\n",
      "    thisModel.fit( pd.DataFrame(train_x[thisCol]), train_y['label'])\n",
      "    thisScore = thisModel.score( pd.DataFrame(test_x[thisCol]), test_y['label'])\n",
      "    # Holding onto the best model outcomes\n",
      "    if thisScore > bestOneScore:\n",
      "        bestOneScore = thisScore\n",
      "        bestOneCol = thisCol"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print bestOneScore, bestOneCol"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.761506276151 F_240\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that you have figured out how to do a single iteration of forward selection (going from 0 -> 1 feature), use the below boiler plate code to complete the algorithm.\n",
      "\n",
      "Since we want to find the best model with 20 features, we loop through feature selection 20 times.  Add the following steps:\n",
      "\n",
      "1. Train models using the features in $f$ + 1 additional feature\n",
      "2. Find the additional feature that results in the highest accuracy\n",
      "3. Add the best addditional feature to $f$ and the accuracy of the best model to $acc$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thisModel = sklearn.linear_model.LogisticRegression() # instantiate the model\n",
      "\n",
      "ff     = [bestOneCol]      # list of features in the final dataframe\n",
      "acc    = [bestOneScore]       # list of cumulative best accuracies\n",
      "\n",
      "while len(ff) < 20:\n",
      "    print \"Last best column:\", ff[-1],\" with accuracy:\", acc[-1]\n",
      "    bestScore = -1.0                 # (re)initialize the best score\n",
      "    for thisCol in train_x.columns:\n",
      "        if thisCol in ff:             # (skip if already in final list)\n",
      "            continue\n",
      "        fftmp = list(ff)        # copy growing column list\n",
      "        fftmp.append(thisCol)   # append this test column\n",
      "        \n",
      "        thisModel.fit(train_x[fftmp], train_y['label'])\n",
      "        thisScore = thisModel.score(test_x[fftmp], test_y['label'])\n",
      "        # Holding onto the best model outcomes for this loop\n",
      "        if thisScore > bestScore:\n",
      "            bestScore = thisScore\n",
      "            bestCol   = thisCol\n",
      "    # done with loop, append best feature and its accuracy to ff,acc\n",
      "    ff.append(bestCol)\n",
      "    acc.append(bestScore)\n",
      "            \n",
      "    #TODO: search for the best feature to add to features to maximize performance\n",
      "    #TODO: add the best feature to features\n",
      "    #TODO: add the accuracy of the best model to acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Current length of ff: 1 F_240 0.761506276151\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 F_368_SQ 0.799163179916\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 F_396 0.832635983264\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 F_169_ABS 0.853556485356\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 F_348 0.866108786611\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 F_711_ABS 0.878661087866\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 F_315 0.887029288703\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 F_178_SQ 0.891213389121\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 F_628_SQ 0.899581589958\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 F_79 0.903765690377\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11 F_244 0.907949790795\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12 F_77_SQ 0.912133891213\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13 F_284_ABS 0.916317991632\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14 F_393_SQ 0.92050209205\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15 F_1 0.92050209205\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 16 F_2 0.92050209205\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 17 F_5 0.92050209205\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18 F_7 0.92050209205\n",
        "Current length of ff:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 19 F_8 0.92050209205\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now plot the accuracy as a function of the number of features.  Also plot a horizontal line showing the accuracy of the baseline model and the original 20 high coefficient feature model.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top20_score\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "0.89121338912133896"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# number of features for acc\n",
      "nfeats = range(1,21)\n",
      "baseline = np.empty(20); baseline.fill(acc_baseline)\n",
      "top20acc = np.empty(20); top20acc.fill(top20_score)\n",
      "\n",
      "legStr = ['Forward Selection', 'Baseline (800 features)', 'Top 20 Coeffs']\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(nfeats, acc, 'bo')\n",
      "plt.plot(nfeats, acc, 'k', label=legStr[0])\n",
      "plt.plot(nfeats, baseline, label=legStr[1])\n",
      "plt.plot(nfeats, top20acc, label=legStr[2])\n",
      "legend(loc='best')\n",
      "plt.title('% Accuracy by Number of Features')\n",
      "plt.xlabel('Number of Features')\n",
      "plt.ylabel('% Accuracy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 100,
       "text": [
        "<matplotlib.text.Text at 0x110803fd0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEZCAYAAACJjGL9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x110a61e90>"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Discussion of Results\n",
      "Now discuss the results.  In particular address the following issues:\n",
      "\n",
      "1. With the feature engineering and forward selection is accuracy better than baseline attainable?  If so how many features are required to beat the baseline?\n",
      "2. Based on the trajectory of the accuracy plot, do you think more features will significantly improve accuracy?\n",
      "3. Do some of the most important features from the baseline model still show up in our final forward selection model?  Are they transformed?\n",
      "4. Do all 3 sets of features (raw, transformed, pca) appear in the final model?  Which set seems to be the most important?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}