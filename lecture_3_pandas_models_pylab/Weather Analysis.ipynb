{
 "metadata": {
  "name": "",
  "signature": "sha256:77fa615580bc014ca283e40d72880ea88e9efe2bc618c335966a84cd8cd657a9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Some Analysis of the NOAA weather dataset ###\n",
      "\n",
      "In this notebook we are analyzing a sample out of data that was downloaded from http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/, the main file is ghcnd_all.tar.gz which is about 2.4 GB which becomes around 20GB when uncompressed.\n",
      "\n",
      "The data contains about 1 million station-year recordings. That is too much to analyzer on single core machine, so we start by taking a sample of 20,000 recordings of the maximal daily temperatures for a period of a 365 days starting on January 1st (the last day of leap years is discarded)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Checking the versions of some important packages ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Switch to the data directory and check it's contents"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd ~/BigData/UCSD_BigData/data/weather/    \n",
      "#%cs ~/data/weather\n",
      "!ls -lh"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat data-source.txt"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "- *data-source.txt* - information about downloading the data from NOAA\n",
      "- *ghcnd-readme.txt* - A readme file describing the content of all of the files from ghcnd, in particular:\n",
      "- *ghcnd-stations.txt* - information about each of the meteorological stations.\n",
      "- *Sample_TMAX_By_Year* - a file with 10,000 randomly selected one-year-long TMAX measurements"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -1 SAMPLE_TMAX.csv\n",
      "# a Typical single line in the data file"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### read data into a Pandas Dataframe ##\n",
      "* Read the data into a DataFrame\n",
      "* Read the data vectors in G\n",
      "* Divide by 10.0 to get the temperatude in degrees celsius\n",
      "* Replace values outside the range [-400,500]  ([-40,50] degrees celsius) with nan  \n",
      "* Paste fixed matrix back into Dout\n",
      "* Show the first few lines of DDout"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "header=['station','measurement','year']+range(1,366)\n",
      "# D=pandas.DataFrame(columns=header)\n",
      "Data = pd.read_csv('SAMPLE_TMAX.csv',header=None,names=header)\n",
      "G=Data.ix[:,1:365]\n",
      "G[G<-400]=nan\n",
      "G[G>500]=nan\n",
      "G=G/10\n",
      "Data.ix[:,1:365]=G\n",
      "G=G.transpose()\n",
      "Data.head()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp=G.ix[:,:].unstack()\n",
      "print shape(tmp), type(tmp)\n",
      "tmp.hist(bins=50);\n",
      "title('overall distribution of temperatures')\n",
      "print tmp.min(),tmp.max()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Script for plotting yearly plots ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import date\n",
      "dates=[date.fromordinal(i) for i in range(1,366)]\n",
      "def YearlyPlots(T,ttl='',size=(10,7)):\n",
      "    fig=plt.figure(1,figsize=size,dpi=300)\n",
      "    #fig, ax = plt.subplots(1)\n",
      "    if shape(T)[0] != 365:\n",
      "        raise ValueError(\"First dimension of T should be 365. Shape(T)=\"+str(shape(T)))\n",
      "    plot(dates,T);\n",
      "    # rotate and align the tick labels so they look better\n",
      "    fig.autofmt_xdate()\n",
      "    ylabel('temperature')\n",
      "    grid()\n",
      "    title(ttl)\n",
      "YearlyPlots(Data.ix[20:30,1:365].transpose(),ttl='A sample of yearly plots')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Plots for sydney, Australia ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sydneyStations=['ASN00066' in station for station in Data['station']]\n",
      "print Data[sydneyStations]['station'].values\n",
      "#for station in sydneyStations:\n",
      "#    print station,sum(Data['station']==station)\n",
      "tmp=Data[sydneyStations].transpose()\n",
      "YearlyPlots(tmp.ix[1:365,:],ttl='Sydney Stations')\n",
      "#tmp.ix[:,tmp.columns[7]]\n",
      "#Data[sydneyStations][['station','year']]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Computing mean and std for each station/year ###\n",
      "And calculating the standard deviation. In this case we are not divi"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a simple scale function to normalize the data-frame row-by-row\n",
      "from numpy import mean, std\n",
      "def scale_temps(Din):\n",
      "    matrix=Din.iloc[:,3:]\n",
      "    Dout=Din.loc[:,['station','year']+range(1,366)]\n",
      "    Mean=mean(matrix, axis=1).values\n",
      "    Dout['Mean']=Mean\n",
      "    Std= std(matrix, axis=1).values\n",
      "    Dout['Std']=Std\n",
      "    # Decided not to normalize each year to have mean zero and std 1\n",
      "    # tmp = pd.DataFrame((matrix.values - Mean[:,np.newaxis])/Std[:,newaxis],columns=range(1,366))\n",
      "    # print tmp.head()\n",
      "    Dout.loc[:,1:365]=matrix.values\n",
      "    return Dout\n",
      "Dout=scale_temps(Data)\n",
      "#reorder the columns\n",
      "Dout=Dout[['station','year','Mean','Std']+range(1,366)]\n",
      "Dout.head()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Compute average temperature for each day of the year. ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Mean=mean(Dout.ix[:,1:365], axis=0)\n",
      "YearlyPlots(Mean,ttl='The global mean temperature for each day of the year')\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "### SVD ###\n",
      "Using a sparse svd solver directly, using https://pypi.python.org/pypi/sparsesvd/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import numpy, scipy.sparse\n",
      "#from sparsesvd import sparsesvd\n",
      "#smat = scipy.sparse.csc_matrix(Data.loc[:,1:365]) # convert to sparse CSC format\n",
      "#ut, s, vt = sparsesvd(smat, 10) # do SVD, asking for 10 factors\n",
      "#print shape(ut),shape(s),shape(vt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Missing Values ###\n",
      "We find the distribution of missing values and decide how to deal with them. From the analysis below we see that most rows have some\n",
      "missing values. We therefor choose to perform the average more carefully, rather than discard rows with many missing values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nan_per_row=sum(isnan(Dout.ix[:,1:365]),axis=1)\n",
      "nan_per_row.hist(bins=100)\n",
      "sum(nan_per_row>50)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### NaN-tolerant averaging  ###\n",
      "We compute the empirical covariance matrix in a way that tolerates NaN values.\n",
      "\n",
      "<span style=\"color:red\"> In the code below I remove all rows that have a nan in them. If you remve the command **M.dropna(...** then all rows are used. Can you get better results without removing the rows? </span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# demonstrating the use of the cell magic %%time, which measures the run-time of the cell.\n",
      "M=Dout.loc[:,1:365].transpose()\n",
      "M=M.dropna(axis=1)\n",
      "(columns,rows)=shape(M)\n",
      "Mean=mean(M, axis=1).values\n",
      "\n",
      "print (columns,rows), shape(Mean)\n",
      "C=np.zeros([columns,columns])   # Sum\n",
      "N=np.zeros([columns,columns])   # Counter of non-nan entries"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "for i in range(rows):\n",
      "    if i % 1000==0: \n",
      "        print i\n",
      "    row=M.iloc[:,i]-Mean;\n",
      "    outer=np.outer(row,row)\n",
      "    valid=isnan(outer)==False\n",
      "    C[valid]=C[valid]+outer[valid]  # update C with the valid location in outer\n",
      "    N[valid]=N[valid]+1\n",
      "valid_outer=np.multiply(1-isnan(N),N>0)\n",
      "cov=np.divide(C,N)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shape(cov)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "U,D,V=np.linalg.svd(cov)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shape(U),shape(D),shape(V)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Percentage of variance Explained ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(cumsum(D[:])/sum(D))\n",
      "xlim([0,365])\n",
      "grid()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k=6 # number of components to show.\n",
      "YearlyPlots((U[:,:k]),ttl='The most significant eigen-vectors')\n",
      "legend(range(0,k));"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# U1,D1,V1=np.linalg.svd(matrix[:100,:])  # running the svd on the whole matrix seems to crash because of NaN entries"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k=50\n",
      "Eig=np.matrix(U[:,:k])\n",
      "print [np.linalg.norm(U[:,i]) for i in range(k)]\n",
      "matrix=np.matrix(Dout.ix[:,1:365])-Mean\n",
      "matrix[isnan(matrix)]=0\n",
      "print shape(Eig),shape(matrix)\n",
      "Prod=matrix*Eig;\n",
      "print shape(Prod)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Insert coefficients for k top eigenvectors into the dataframe **Dout**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(k-1,-1,-1):\n",
      "    Ser=pd.Series(np.array(Prod)[:,i],index=Dout.index)\n",
      "    Dout.insert(4,'V'+str(i),Ser)\n",
      "Dout.head()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Longitude,Latitude information ###\n",
      "Loading the station longitude/latitude and merging it into the Table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ghcnd-readme.txt   # uncomment to read the readme file."
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "------------------------------\n",
      "Variable   Columns   Type\n",
      "------------------------------\n",
      "ID            1-11   Character\n",
      "LATITUDE     13-20   Real\n",
      "LONGITUDE    22-30   Real\n",
      "ELEVATION    32-37   Real\n",
      "STATE        39-40   Character\n",
      "NAME         42-71   Character\n",
      "GSNFLAG      73-75   Character\n",
      "HCNFLAG      77-79   Character\n",
      "WMOID        81-85   Character\n",
      "------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make all lines be of length 90 to solve problem wilth read_fwf\n",
      "out=open('ghcnd-stations_buffered.txt','w')\n",
      "for line in open('ghcnd-stations.txt','r').readlines():\n",
      "    line=line.rstrip()\n",
      "    string=line+' '*(90-len(line))+'\\n'\n",
      "    out.write(string)\n",
      "out.close()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "A grid to define the character locations of the fields\n",
      "\n",
      "AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\n",
      "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\n",
      "0         1         2         3         4         5         6         7         8         9"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "colspecs = [(0, 11), (11, 21), (21, 31), (31, 38),(39,41),(41,72),(72,76),(76,80),(80,86)]\n",
      "stations = pd.read_fwf('ghcnd-stations_buffered.txt', colspecs=colspecs, header=None, index_col=0,\n",
      "                       names=['latitude','longitude','elevation','state','name','GSNFLAG','HCNFLAG','WMOID'])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#stations['elevation'][stations['elevation']==-999.9]=0  # decided not to remove -999.9 because this confuses hist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stations.head()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### perform a **JOIN** ###\n",
      "Join the geographical information into **Dout**, creating a new dataframe called **Djoined**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Djoined=Dout.join(stations,on='station')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Djoined.columns[-10:]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Djoined['AbsLatitude']=abs(Djoined['latitude'].values)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Djoined.ix[:5,['station',u'longitude','latitude',u'AbsLatitude','Mean','Std','V0','V1','V2']]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Looking for significant correlations and dependencies ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Djoined[['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4','V5']].cov()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<span style=\"color:red\"> The correlations between different $V_i$ components should be zero, which it isn't.\n",
      "Is this due to numerical roundoff errors? Are the correlations statistically significant for this sample size? </span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Djoined[['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4','V5']].corr()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Choosing significance threshold so that none of the correlations between the Vi-s are significant.\n",
      "abs(Djoined[['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4','V5']].corr())>0.2"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas.tools.plotting import scatter_matrix\n",
      "df = Djoined.ix[:,['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4','V5']]\n",
      "scatter_matrix(df, alpha=0.03, figsize=(20, 20), diagonal='kde');"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X='latitude'\n",
      "Djoined.ix[:,X].hist(bins=100);"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Experimenting with RPlot scatter plots ###\n",
      "The advantage is that you can get both the scatter points and the topographic density estimate, which is useful when you have a large number of data points. However, \n",
      "I don't know how to increase the number of topo-lines to see the lower density areas. <span style=\"color:red\">Can you fix this problem? Can you write code to create a Scatter matrix using rplot? </span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Taken from http://pandasplotting.blogspot.com/\n",
      "import matplotlib.pyplot as plt\n",
      "from pandas.tools import rplot\n",
      "#plt.figure()\n",
      "X='latitude';Y='Mean'\n",
      "dfTmp=Djoined[[X,Y]].dropna()\n",
      "dfTmp=dfTmp.iloc[:,:]\n",
      "p = rplot.RPlot(dfTmp,x=X,y=Y)\n",
      "p.add(rplot.GeomPoint())\n",
      "p.add(rplot.GeomDensity2D())\n",
      "p.render();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To see the source of a method use ??\n",
      "rplot.GeomDensity2D??"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X='latitude';Y='Mean'\n",
      "scatter(Djoined.loc[:,X],Djoined.loc[:,Y],alpha=0.05)\n",
      "xlabel(X)\n",
      "ylabel(Y)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#checking for an anomaly in the elevations of stations\n",
      "Djoined[['station','elevation']][Djoined['elevation']<-500].head()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!grep ASN00010865 ghcnd-stations.txt"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Plotting maps ###\n",
      "Working through http://matplotlib.org/basemap/\n",
      "\n",
      "* http://en.wikipedia.org/wiki/Map_projection\n",
      "* http://matplotlib.org/basemap/users/mapsetup.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lons=stations.ix[:,'longitude'].values\n",
      "lats=stations.ix[:,'latitude'].values\n",
      "station_names=stations.index.values\n",
      "ll=len(lons)\n",
      "lonmin=-180;lonmax=180;latsmin=-80;latsmax=80;\n",
      "select=(lons>lonmin) * (lons<lonmax)*(lats>latsmin)*(lats<latsmax)\n",
      "print sum(select)\n",
      "station_names=station_names[select]\n",
      "lons=lons[select]\n",
      "lats=lats[select]\n",
      "print len(lons),len(lats),len(station_names)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://matplotlib.org/basemap/users/merc.html\n",
      "\n",
      "from mpl_toolkits.basemap import Basemap\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "# llcrnrlat,llcrnrlon,urcrnrlat,urcrnrlon\n",
      "# are the lat/lon values of the lower left and upper right corners\n",
      "# of the map.\n",
      "# lat_ts is the latitude of true scale.\n",
      "# resolution = 'i' means use intermediate resolution coastlines.\n",
      "plt.figure(figsize=(15,10),dpi=300)\n",
      "m = Basemap(projection='merc',llcrnrlat=latsmin,urcrnrlat=latsmax,\\\n",
      "            llcrnrlon=lonmin,urcrnrlon=lonmax,lat_ts=20,resolution='i')\n",
      "m.drawcoastlines()\n",
      "m.fillcontinents(color='coral',lake_color='aqua')\n",
      "\n",
      "# draw parallels and meridians.\n",
      "parallels = np.arange(-80,81,10.)\n",
      "# labels = [left,right,top,bottom]\n",
      "m.drawparallels(parallels,labels=[False,True,True,False])\n",
      "meridians = np.arange(10.,351.,20.)\n",
      "m.drawmeridians(meridians,labels=[True,False,False,True])\n",
      "\n",
      "#m.drawparallels(np.arange(-90.,91.,30.))\n",
      "#m.drawmeridians(np.arange(-180.,181.,60.))\n",
      "m.drawmapboundary(fill_color='aqua')\n",
      "\n",
      "# draw map with markers for locations\n",
      "x, y = m(lons,lats)\n",
      "m.plot(x,y,'g.')\n",
      "\n",
      "plt.title('weather stations')\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get to these coordinate on Google Maps, type the latitude and longitude in decimal in the search box or use:\n",
      "https://www.google.com/maps/place/72%C2%B018'00.0%22S+170%C2%B013'00.1%22E/@-72.3,170.216694,17z/data=!3m1!4b1!4m2!3m1!1s0x0:0x0\n",
      "\n",
      "<span style=\"color:red\">HW questions</span>\n",
      "\n",
      "1. Waiting for somebody to write a script that will do that automatically from python\n",
      "2. Can you create a map where the denity of points is represented as a density map (topographical map)?\n",
      "3. Can you create a map that would represent, using color, the values of a chosen column (Mean, Std, V0,V1 etc.)? What conclusions can you draw from this map?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Reconstruction ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_reconstructions(selection,rows=2,columns=7):\n",
      "    Recon=Eig*Prod.transpose()+Mean[:,np.newaxis]\n",
      "    plt.figure(figsize=(columns*3,rows*3),dpi=300)\n",
      "    j=0;\n",
      "    for i in selection:\n",
      "        subplot(rows,columns,j); \n",
      "        j += 1; \n",
      "        if j>=rows*columns: break\n",
      "        plot(Recon[:,i])\n",
      "        plot(Djoined.ix[i,1:365]);\n",
      "        title(Djoined.ix[i,'station']+' / '+str(Djoined.ix[i,'year']))\n",
      "        xlim([0,365])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Observe in the reconstructions below that the bloue line fills in (extrapolation/interpolation) the places where the measurements are not available. It also reduces the fluctuations in the relative to the original line. Recall the we are using the k top eigenvectors which explain about 88% of the variance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_reconstructions(range(50,100),rows=7,columns=3)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\"> Check how the approximations change/improve as you increase the number of coefficients</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hist(Djoined.ix[:,'V0'],bins=100);"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "selection= [i for i in range(shape(Djoined)[0]) if Djoined.ix[i,'latitude']<-10]\n",
      "plot_reconstructions(selection,rows=7,columns=3)\n",
      "shape(selection)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">Can you reduce the reconstruction error (using a fixed number of eigenvectors) by splitting the stations according to region (for example country, state, latitudal range). Note that having a regions with very few readings defeats the purpose."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}