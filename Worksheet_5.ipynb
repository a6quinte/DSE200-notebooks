{
 "metadata": {
  "name": "",
  "signature": "sha256:a7118c0a0a2308b9c40028217e7eb5542803e448e3aa2047d44298363e84d7bb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Generative Models 1: \n",
      "#Naive Bayes Classification of Text\n",
      "http://qwone.com/~jason/20Newsgroups/\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**To Do:**\n",
      " - Compare uncleaned text files to cleaned files\n",
      " - Inspect cleaned files, document columns\n",
      " - Create 20 different probability vectors using **train** data set\n",
      " - Implement MaxArg to classify **test** data set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import log10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Define directory and file names"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# working directory\n",
      "data_dir = '/Users/a1quintero/Documents/DSE/DSE210-Stats/HW2/20news-clean/'\n",
      "voc_filename = data_dir + 'vocabulary.txt'\n",
      "\n",
      "# train data filenames\n",
      "dat_trainname = data_dir + 'train.data'\n",
      "lab_trainname = data_dir + 'train.label'\n",
      "map_trainname = data_dir + 'train.map'\n",
      "\n",
      "# test data filenames\n",
      "dat_testname = data_dir + 'test.data'\n",
      "lab_testname = data_dir + 'test.label'\n",
      "map_testname = data_dir + 'test.map'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Read Vocabulary and Train Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# READING FILES\n",
      "with open(voc_filename,'r') as ff:           ## vocabulary set\n",
      "    voc_arr = ff.read().splitlines()\n",
      "\n",
      "with open(map_trainname,'r') as ff:          ## map [className, classID]\n",
      "    map_arr = ff.read().splitlines()\n",
      "    \n",
      "with open(lab_trainname,'r') as ff:          ## label: [classLabel] (row number is doc_IDx)\n",
      "    lab_arr = ff.read().splitlines()\n",
      "\n",
      "with open(dat_trainname,'r') as ff:          ## data [ docID, wordID, count ]\n",
      "    dat_arr = ff.read().splitlines()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Data File descriptions:\n",
      "**{train,test}.data files:** ---   \"doc_IDx | word_IDx | count\"\n",
      "\n",
      "**{train,test}.map files:** --- \"list of Classes. line number corresponds to doc_IDx\"\n",
      "\n",
      "**{train,test}.label files:** --- \"ClassName | ClassID\"\n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generate Laplace Smoothed dictionaries of the vocabulary set\n",
      "\n",
      "Here we create an array of dictionaries--one dictionary for each class. The keys are the index number (starting from 0) of the vocabulary file.  The values are the counts for the given word.  To implement **Laplace Smoothing**, I initialize all counts to 1 instead of 0.\n",
      "\n",
      "**NOTE:** These dictionaries are *counts*, the integer number of words in each class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab_len = len(voc_arr) # length of vocabulary set\n",
      "n_groups  = len(map_arr) # number of groups/classes\n",
      "n_docs    = len(lab_arr) # number of unique documents\n",
      "\n",
      "# generate dictionary of counts, all seen one time (laplace smoothed)\n",
      "freq_dict_arr = []\n",
      "for ii in range( n_groups ):\n",
      "    freq_dict_arr.append( dict((jj,float(1)) for jj in range(vocab_len)) )\n",
      "    \n",
      "# populating the frequency array\n",
      "for dat_row in dat_arr:\n",
      "    row_arr = dat_row.split()\n",
      "    doc_id  = int( row_arr[0] )\n",
      "    classIndx = int(lab_arr[doc_id-1]) - 1    # class index of this line\n",
      "    word_id = int( row_arr[1] )\n",
      "    wordIndx  = word_id - 1                   # get wordID (key) of this line\n",
      "    word_count = int(row_arr[2])              # frequency in this article\n",
      "    freq_dict_arr[classIndx][wordIndx] += word_count # burstiness correction to go here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Here we define two functions that we use below:\n",
      "\n",
      " - **normDictVals** - Takes an input dictionary and returns a normalized version\n",
      "\n",
      " - **nb_classifier** - Takes arrays of pi values and trained class dictionaries (both of same length, 20 in this case) as well as the dictionary of the document to be classified.  It returns the index of the predicted class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Functions used in this notebook\n",
      "\n",
      "def normDictVals(inDict):\n",
      "    \"\"\"\n",
      "    INPUT: \n",
      "       inDict - dictionary with numeric values\n",
      "    OUTPUT:\n",
      "       outDict\n",
      "    \"\"\"    \n",
      "    tot_sum = float( sum(inDict.values()) )\n",
      "    outDict = inDict.copy()\n",
      "    outDict.update((x,y/tot_sum) for x, y in outDict.items())\n",
      "    return outDict\n",
      "\n",
      "\n",
      "def nb_classifier(piArr, x_dict, p_arr):\n",
      "    \"\"\"\n",
      "    INPUT: \n",
      "       piArr  - array of 20 percentages of each class's prob \n",
      "       x_dict - dictionary of word counts (wordID,count)\n",
      "       p_arr  - array of 20 dictionaries \n",
      "    OUTPUT:\n",
      "       Index of predicted class\n",
      "    \"\"\"\n",
      "    max_prob = -1e10\n",
      "    best_class = -1\n",
      "    for ii in range( len(piArr) ):  # ii: 0:19\n",
      "        # getting the result of the summation\n",
      "        current_sum = 0.0\n",
      "        for wID,xx in x_dict.items():             # loop over words in dictionary\n",
      "            pp = p_arr[ii][wID]\n",
      "            current_sum += xx*log10(pp)#+(1-xx)*log10(1-pp)\n",
      "        this_prob = log10(piArr[ii]) + current_sum\n",
      "        if this_prob > max_prob:\n",
      "            max_prob = this_prob\n",
      "            best_class = ii\n",
      "    \n",
      "    return best_class"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Prepare the needed values for classifying\n",
      "\n",
      "Here we convert the frequency dictionary arrays to *probability* dictionary arrays.  The sum of each dictionary's values sum to unity.\n",
      "\n",
      "We also create an array of pi values for each class.  They are the percent of documents in the training set that are assigned to each class.  These sum to unity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# preparing the needed values for the classification\n",
      "#  - getting array of pi vals (percent of each clas)\n",
      "#  - changing the frequency array to prob. array \n",
      "#    ( each dictionary sums to one)\n",
      "\n",
      "pi_arr = []            # fraction of articles in each class\n",
      "prob_dict_arr = []     # probability array of dictionaries\n",
      "\n",
      "# normalizing dictionaries and getting each pi value\n",
      "for ii in range(n_groups):\n",
      "    pi_arr.append( float(lab_arr.count(str(ii+1)))/float(n_docs) )  # pi=>%of total articles\n",
      "    thisdict = normDictVals(freq_dict_arr[ii]) # normalizing dictionary to unity\n",
      "    prob_dict_arr.append(thisdict)\n",
      "\n",
      "# [sum(ii.values()) for ii in prob_dict_arr]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Read test data\n",
      "Now we're ready to classify the test data..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read test data\n",
      "with open(map_testname,'r') as ff:         ## map [className, classID]\n",
      "    map_test_arr = ff.read().splitlines()\n",
      "with open(lab_testname,'r') as ff:         ## label: [classLabel] (row number is doc_IDx)\n",
      "    lab_test_arr = ff.read().splitlines()\n",
      "with open(dat_testname,'r') as ff:         ## data [ docID, wordID, count ]\n",
      "    dat_test_arr = ff.read().splitlines()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Run the classification\n",
      "\n",
      "Here we run the classification.  For each document, we only hold onto the word index values that are present.  This saves time when running."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# running the classification on the test set\n",
      "# get a vector for each document, holding onto nonzero values\n",
      "# take argument maximum to get class\n",
      "\n",
      "last_doc = int(dat_test_arr[0].split()[0])  # getting the docID of first line\n",
      "this_doc_dict = {}\n",
      "\n",
      "mapped_class_arr = []\n",
      "\n",
      "for dat_row in dat_test_arr:\n",
      "    \n",
      "    # read data\n",
      "    row_arr = dat_row.split()\n",
      "    doc_id  = int( row_arr[0] )\n",
      "    classIndx = int(lab_arr[doc_id-1]) - 1  # class index of this line\n",
      "    word_id = int( row_arr[1] )\n",
      "    wordIndx  = word_id - 1                 # get wordID (key) of this line\n",
      "    word_count = float(row_arr[2])          # frequency in this article\n",
      "    \n",
      "    if last_doc == doc_id:\n",
      "        this_doc_dict[wordIndx] = word_count  # add this word to this document\n",
      "\n",
      "    else:      # new document\n",
      "        \n",
      "        # predict this document's class\n",
      "        mapped_class_arr.append( nb_classifier(pi_arr, this_doc_dict, prob_dict_arr) )\n",
      "\n",
      "        # reset dict for new doc, add first item\n",
      "        this_doc_dict = {}\n",
      "        this_doc_dict[wordIndx] = word_count  # add first word to this document\n",
      "    \n",
      "    last_doc = doc_id\n",
      "\n",
      "# map the final document\n",
      "mapped_class_arr.append( nb_classifier(pi_arr, this_doc_dict, prob_dict_arr) )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Analysis\n",
      "Now we analyze the results and make a confusion matrix to analyze the results.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conf_mat = []\n",
      "for ii in range(n_groups):\n",
      "    conf_row = []\n",
      "    for jj in range(n_groups):\n",
      "        conf_row.append(0)\n",
      "    conf_mat.append(conf_row)\n",
      "\n",
      "ncorrect = 0.0\n",
      "for class_actual, class_mapped in zip(lab_test_arr, mapped_class_arr):\n",
      "    actual_val = int(class_actual)\n",
      "    mapped_val = int(class_mapped + 1)\n",
      "    conf_mat[mapped_val-1][actual_val-1] += 1\n",
      "    if actual_val == mapped_val:        \n",
      "        ncorrect += 1\n",
      "\n",
      "        \n",
      "        \n",
      "print float(ncorrect)/float(len(mapped_class_arr))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.78107928048\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conf_mat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[[235, 3, 3, 0, 0, 0, 0, 1, 0, 4, 2, 0, 2, 10, 3, 7, 1, 12, 6, 47],\n",
        " [0, 296, 33, 8, 8, 42, 8, 1, 1, 1, 0, 4, 18, 7, 7, 2, 0, 1, 1, 3],\n",
        " [0, 6, 207, 15, 10, 7, 4, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
        " [0, 12, 58, 305, 37, 10, 50, 2, 0, 1, 0, 1, 27, 3, 0, 0, 0, 0, 0, 0],\n",
        " [0, 7, 11, 21, 273, 2, 20, 0, 0, 1, 0, 2, 8, 0, 0, 1, 1, 0, 0, 0],\n",
        " [1, 22, 31, 2, 3, 306, 1, 1, 0, 2, 0, 1, 3, 0, 2, 2, 0, 0, 1, 0],\n",
        " [0, 1, 0, 4, 4, 1, 226, 5, 0, 3, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0],\n",
        " [0, 3, 2, 5, 4, 0, 33, 356, 26, 3, 1, 0, 10, 4, 0, 0, 2, 2, 1, 0],\n",
        " [0, 2, 2, 0, 1, 2, 5, 4, 353, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
        " [0, 0, 2, 0, 1, 1, 0, 2, 2, 345, 4, 0, 0, 1, 0, 0, 1, 1, 0, 0],\n",
        " [1, 0, 1, 1, 0, 0, 1, 0, 0, 17, 381, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
        " [1, 17, 17, 6, 6, 10, 3, 1, 1, 2, 1, 361, 46, 1, 4, 1, 3, 4, 3, 1],\n",
        " [1, 4, 1, 23, 17, 0, 11, 4, 1, 2, 0, 3, 259, 3, 4, 0, 0, 0, 0, 0],\n",
        " [2, 4, 4, 0, 8, 0, 2, 0, 1, 0, 2, 2, 6, 324, 4, 1, 1, 0, 3, 3],\n",
        " [3, 7, 4, 1, 2, 3, 3, 2, 0, 0, 1, 0, 3, 3, 334, 0, 2, 0, 7, 5],\n",
        " [45, 4, 5, 0, 0, 2, 4, 1, 1, 3, 2, 2, 6, 17, 5, 377, 3, 8, 3, 71],\n",
        " [3, 0, 0, 1, 3, 1, 2, 4, 4, 1, 0, 8, 0, 3, 1, 2, 324, 3, 95, 19],\n",
        " [10, 0, 0, 0, 0, 1, 3, 2, 2, 2, 1, 0, 2, 6, 2, 2, 3, 326, 5, 5],\n",
        " [7, 1, 9, 0, 6, 2, 6, 9, 5, 9, 3, 8, 0, 10, 23, 1, 16, 17, 184, 8],\n",
        " [9, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 4, 0, 1, 88]]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "# from pylab import *\n",
      "# imshow(conf_mat)\n",
      "# show()\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "fig = plt.figure()\n",
      "plt.clf()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_aspect(1)\n",
      "res = ax.imshow(np.array(conf_mat), cmap=plt.cm.jet, \n",
      "                interpolation='nearest')\n",
      "width = len(conf_mat)\n",
      "height = len(conf_mat[0])\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['log10']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD7CAYAAABOrvnfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x11cba1dd0>"
       ]
      }
     ],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}